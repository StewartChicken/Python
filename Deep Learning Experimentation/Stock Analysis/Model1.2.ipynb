{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61941aa-b217-4733-8bb6-ba93bd18b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "with open(\"sp500tickers_unrevised.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "       \n",
    "\n",
    "    \n",
    "dropped_tickers = ('BALL', 'BRK-B', 'BF-B', 'KMX', 'WBD', 'CEG', 'PARA', 'WTW', 'META', 'OGN', 'BBWI')\n",
    "\n",
    "for i in dropped_tickers:\n",
    "    if i in tickers:\n",
    "        del tickers[tickers.index(i)]\n",
    "        \n",
    "hm_units = 10\n",
    "req = 0.002\n",
    "SEQ_LEN = 60\n",
    "BATCH_SIZE = 132\n",
    "EPOCHS = 30\n",
    "NAME = f'{SEQ_LEN}-SEQ-{hm_units}-PRED-{int(time.time())}'\n",
    "CORRELATION_COEFFICIENT = 0.92\n",
    "\n",
    "#api keys that will be used to access data from the source\n",
    "        \n",
    "api = 'gDvQNVWDC2mhOlN1Z9if6JEyDM08CpeC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea4bee9-9460-47ba-ad11-3a7d1516b0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function determines whether the model should buy or sell shares of a company. \n",
    "#One of three values will be returned, either a -1, 0, or 1\n",
    "#If a -1 is returned, that means that the model predicts a percent decrease in price, \n",
    "#equivalant in magnitude to the 'req' variable, over the next (hm_units) candles, and it\n",
    "#also predicts that the price will *not* increase in value by a significant degree before\n",
    "#it reaches the prdicted candle. (ie, the model tries to predict instances of increase/decrease\n",
    "#that will not result in the user being 'wicked out' of the trade, should the price fluctuate \n",
    "#between low and high values very aggressively.\n",
    "\n",
    "#The function takes two parameters: the current price, and the list of future prices the immediately follow the\n",
    "#current price. This 'future_prices' list will be exactly 'hm_units' long.\n",
    "def classify(current_price, future_price, future_lows):\n",
    "    \n",
    "    #Sets the threshold value for when a price movement is considered to be significant \n",
    "    requirement = req\n",
    "    \n",
    "    #Creates a list of %changes in relation to 'current_price' for each future price within 'future_prices' \n",
    "    lows = []\n",
    "    for price in future_lows:\n",
    "        lows.append((price - current_price) / current_price)\n",
    "        \n",
    "    future_change = (future_price - current_price) / current_price\n",
    "    \n",
    "    #If the last value (value to be predicted) is greater than or equal to the threshold (a significant increase is predicted),\n",
    "    #the method will prepare to return either a 1 (significant increase w/o prior decrease) or a 0 (significant increase *with*\n",
    "    #prior decrease\n",
    "    if future_change >= requirement:\n",
    "        \n",
    "        #Checks to see if any of the values in the 'futures' list (excluding the very last value (prediction value))\n",
    "        #fall below the requirement (hit the stop loss of the trade before the shares are sold)\n",
    "        for low in future_lows:\n",
    "            if low <= -requirement:\n",
    "                return 0\n",
    "            \n",
    "        return 1\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    #If, in 'hm_units' candles, there are no significant increases or decreases, the function simply returns a zero\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66993846-fa00-4fc7-87a1-d7e561b756bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the ATR of a specific stock at any given time\n",
    "def Minute_ATR_Column(ticker):\n",
    "    df = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "    #df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df.drop([f'{ticker}_Open', f'{ticker}_Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    atrs = []\n",
    "    true_ranges = []\n",
    "    \n",
    "    df[f'{ticker}_Close'] = (df[f'{ticker}_Close'].shift(1))\n",
    "    \n",
    "    true_ranges.append(df[f'{ticker}_High'].iloc[0] - df[f'{ticker}_Low'].iloc[0])\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        ranges = []\n",
    "        ranges.append(df[f'{ticker}_High'].iloc[i] - df[f'{ticker}_Low'].iloc[i])\n",
    "        ranges.append(abs(df[f'{ticker}_High'].iloc[i] - df[f'{ticker}_Close'].iloc[i]))\n",
    "        ranges.append(abs(df[f'{ticker}_Low'].iloc[i] - df[f'{ticker}_Close'].iloc[i]))\n",
    "        true_ranges.append(max(ranges))\n",
    "    \n",
    "    TRS = {'Ranges': true_ranges, 'Date': df.index.values}\n",
    "    true_ranges_DF = pd.DataFrame(data = TRS)\n",
    "    true_ranges_DF.set_index('Date', inplace = True)\n",
    "    \n",
    "    #df = df.join(true_ranges_DF, how = 'outer')\n",
    "    df['ATRS'] = true_ranges_DF['Ranges'].rolling(window = 14, min_periods = 0).sum().div(14)\n",
    "    return df['ATRS']\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99021d49-c4f4-4cdb-8d3b-2429dd93c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_averages_column(ticker, daily = False, minute = False):\n",
    "    \n",
    "    if daily:\n",
    "        dfd = pd.read_csv(f'DAILY_STOCK_DATA//{ticker}.csv') #.format(ticker.replace('.', '-')))\n",
    "        dfd = dfd[f'{ticker}_Close'].to_frame()\n",
    "        dfd[f'{ticker}_Daily_MA_9'] = dfd[f'{ticker}_Close'].rolling(window = 9, min_periods = 0).mean()\n",
    "        dfd[f'{ticker}_Daily_MA_50'] = dfd[f'{ticker}_Close'].rolling(window = 50, min_periods = 0).mean()\n",
    "        dfd[f'{ticker}_Daily_MA_100'] = dfd[f'{ticker}_Close'].rolling(window = 100, min_periods = 0).mean()\n",
    "        dfd[f'{ticker}_Daily_MA_200'] = dfd[f'{ticker}_Close'].rolling(window = 200, min_periods = 0).mean()\n",
    "    \n",
    "        return dfd[f'{ticker}_Daily_MA_9'], dfd[f'{ticker}_Daily_MA_50'], dfd[f'{ticker}_Daily_MA_100'], dfd[f'{ticker}_Daily_MA_200']\n",
    "    \n",
    "    if minute:\n",
    "        dfm = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv') #.format(ticker.replace('.', '-')))\n",
    "        dfm = dfm[f'{ticker}_Close'].to_frame()\n",
    "        dfm[f'{ticker}_Minute_MA_9'] = dfm[f'{ticker}_Close'].rolling(window = 9, min_periods = 0).mean()\n",
    "        dfm[f'{ticker}_Minute_MA_50'] = dfm[f'{ticker}_Close'].rolling(window = 50, min_periods = 0).mean()\n",
    "        dfm[f'{ticker}_Minute_MA_100'] = dfm[f'{ticker}_Close'].rolling(window = 100, min_periods = 0).mean()\n",
    "        dfm[f'{ticker}_Minute_MA_200'] = dfm[f'{ticker}_Close'].rolling(window = 200, min_periods = 0).mean()\n",
    "    \n",
    "        return dfm[f'{ticker}_Minute_MA_9'], dfm[f'{ticker}_Minute_MA_50'], dfm[f'{ticker}_Minute_MA_100'], dfm[f'{ticker}_Minute_MA_200']\n",
    "    \n",
    "#moving_averages_column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a151834-139f-45f9-9d00-5481ef9faded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will get the daily and minute data for a specific ticker, and it will save it to a CSV to be accessed later.\n",
    "#The desired ticker must be passed as an argument, as well as the start/end date of the data (string). \n",
    "#The 'ignore_after_hours' parameter (boolean) will dictate whether the dataset only contains active hours data, \n",
    "#or if it includes pre/post market. If the pre/post market data is also taken, the user must account for \n",
    "#a high volume of zero volume periods, since the stocks are traded less frequently during these times. \n",
    "#The 'minute' and 'daily' paramaters are booleans which dicate which type of data will be acquired:\n",
    "#daily data or minute data. Both initially set to True\n",
    "\n",
    "#Dates must be in format yyyy-mm-dd\n",
    "def get_ticker_data(ticker, start_date, end_date, ignore_after_hours = True, include_minute = True, include_daily = True):\n",
    "    \n",
    "        print(f'Getting {ticker} data')\n",
    "    \n",
    "        #The 'sort' variable dictates the method by which the data is sorted. asc (Ascending) means the oldest dates are at the top\n",
    "        #and desc (Descending) means the newest dates are at the top. DO NOT CHANGE. This variable was only put in place to make it easier\n",
    "        #for me to develop/test the code while I was writing it. \n",
    "        sort = 'asc' #or desc\n",
    "        \n",
    "        #This variable dictates how many days the 'date_tracker' objecd (created below) will be advanced by each time new data is added\n",
    "        #To the stock minute dataframe\n",
    "        increment = 60\n",
    "        \n",
    "        #This is the file path to store the final Minute stock data\n",
    "        minute_data_path = f'MINUTE_STOCK_DATA//{ticker}'\n",
    "        \n",
    "        #This is the file path to store the final Daily stock data\n",
    "        daily_data_path = f'DAILY_STOCK_DATA//{ticker}'\n",
    "        \n",
    "        #Creates 'start'/'end' strings from the 'start_date' and 'end_date' parameters that can be accessed during execution of the method\n",
    "        start = start_date\n",
    "        end = end_date\n",
    "        \n",
    "        #Creates a datetime object that stores the 'end' date (to be compared to during the while loop)\n",
    "        end_date_time = datetime.datetime(int(end[:4]), int(end[5:7]), int(end[8:10]))\n",
    "        \n",
    "        #This 'date_tracker' object will allow the following while loop determine when to exit the loop\n",
    "        date_tracker = datetime.datetime(int(start[:4]), int(start[5:7]), int(start[8:10]))\n",
    "        \n",
    "        #Url string for the initial 'df1' data\n",
    "        minute_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{str(date_tracker)[:10]}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "        \n",
    "        #If the 'minute' parameter is set to True\n",
    "        if include_minute:\n",
    "    \n",
    "            #Requests the minute stock data from the url listed above and creates the proper dataframe    \n",
    "            firstData = requests.get(minute_url).json()\n",
    "            df1 = pd.DataFrame(firstData['results'])\n",
    "            df1.reset_index(inplace = True)\n",
    "            df1.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "\n",
    "            df1['t'] = pd.to_datetime(df1['t'], unit = 'ms')\n",
    "            df1.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                 'o': f'{ticker}_Open', \n",
    "                                 'h': f'{ticker}_High',\n",
    "                                 'l': f'{ticker}_Low',\n",
    "                                 'v': f'{ticker}_Volume',\n",
    "                                 't': 'Date'}, inplace = True)\n",
    "            \n",
    "            #List of features contained within the initial stock data\n",
    "            features_list = []\n",
    "\n",
    "            #Populates the list of features\n",
    "            for col in df1.columns:\n",
    "                features_list.append(col)\n",
    "\n",
    "            #The list of lists containing data from each feature within the minute stock data. The index of each list will \n",
    "            #correspond with the index of the the list's data's respective feature within the 'features' list\n",
    "            df1_data_list = []\n",
    "\n",
    "            #Appends a list form of each column (feature) of data to the 'df1_data_list'\n",
    "            for col in df1.columns:\n",
    "                df1_data_list.append(df1[col].to_list())\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Collecting and compiling data\")'''\n",
    "\n",
    "            #Since each data call is limited to 50000 candles, this while loop is needed to join multiple 50000 candle chunks of data together\n",
    "            #The 'date_tracker' object will tell the while loop when it should exit the loop, and that is when the 'date-tracker' object's date\n",
    "            #Surpasses the requested end_date_time minus seventy days. The reason the seventy is subtracted, is so that an error is not thrown \n",
    "            #when the date_tracker (which is continually incremented by 70 days throughout the loop) surpasses the requested end_date_time, \n",
    "            #thereby possibly requesting data from the future (which obviously doesn't exist)\n",
    "            while date_tracker < (end_date_time - datetime.timedelta(days = increment)):\n",
    "\n",
    "                #Prints the first date of each obtained chunk of data (since data can only be collected in chunks of 50000)\n",
    "                #print(date_tracker)\n",
    "\n",
    "                #Second url that grabs the data that will be added to the end of the initial 'df1' dataframe\n",
    "                minute_url2 = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{str(date_tracker + datetime.timedelta(days = increment))[:10]}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "\n",
    "                #Acquires the 'to-be-added' data and formats it accordingly\n",
    "                secondData = requests.get(minute_url2).json()\n",
    "                df2 = pd.DataFrame(secondData['results'])\n",
    "                df2.reset_index(inplace = True)\n",
    "                df2.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "                df2['t'] = pd.to_datetime(df2['t'], unit = 'ms')\n",
    "                df2.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                     'o': f'{ticker}_Open', \n",
    "                                     'h': f'{ticker}_High',\n",
    "                                     'l': f'{ticker}_Low',\n",
    "                                     'v': f'{ticker}_Volume',\n",
    "                                     't': 'Date'}, inplace = True)\n",
    "\n",
    "                #Creates a list that will contain list formats of each features' data from the 'df2' dataframe\n",
    "                df2_data_list = []\n",
    "\n",
    "                #Populates the above list\n",
    "                for col in df2.columns:\n",
    "                    df2_data_list.append(df2[col].to_list())\n",
    "\n",
    "                #Appends the contents of the newly aqcuired list to the initial list\n",
    "                for i in range(len(features_list)):\n",
    "                    df1_data_list[i].extend(df2_data_list[i])\n",
    "\n",
    "                #Increments the date_tracker object by 'increment' days\n",
    "                date_tracker += datetime.timedelta(days = increment)\n",
    "\n",
    "            #Progress tracker for the progression of the below code\n",
    "            progress = 0.0\n",
    "\n",
    "            #If the 'ignore_after_hours' paramater is read as True, excecute the following code\n",
    "            if ignore_after_hours:\n",
    "\n",
    "                #List containing all of the indeces that are to be deleted\n",
    "                indeces_to_delete = []\n",
    "\n",
    "                #Status update\n",
    "                '''print(\"Finding all pre/post market data\")'''\n",
    "\n",
    "                #This loop will iterate through every data point in the data and determine the indeces of all \n",
    "                #pre/post market data\n",
    "                for i in range(len(df1_data_list[features_list.index('Date')])):\n",
    "\n",
    "                    #Prints the progress of this loop\n",
    "                    '''if(progress % 50000 == 0):\n",
    "                        print(progress / float(len(df1_data_list[features_list.index('Date')])))\n",
    "\n",
    "                    progress += 1'''\n",
    "\n",
    "                    #Sets the hour/minute values of each iteration\n",
    "                    hour = int(str(df1_data_list[features_list.index('Date')][i])[11:13])\n",
    "                    minute = int(str(df1_data_list[features_list.index('Date')][i])[14:16])\n",
    "\n",
    "                    #Appends the index of all of the candles that occur before 1:30 pm and after\n",
    "                    #8:00 pm (since those are the beginning and end times for the recorded stock data\n",
    "\n",
    "                    if hour < 13:\n",
    "                        indeces_to_delete.append(i)\n",
    "                    elif hour >= 20:\n",
    "                        indeces_to_delete.append(i)\n",
    "                    elif hour == 13 and minute < 30:\n",
    "                        indeces_to_delete.append(i)\n",
    "\n",
    "                #Status update\n",
    "                '''print(\"Deleting all pre/post market data\")'''\n",
    "\n",
    "                #This variable accounts for the shift in the data that occurs every time an index is deleted.\n",
    "                #The 'indeces_to_delete' list contains all of the indeces of candles that occur during pre/post market,\n",
    "                #but every time one of those is deleted in the loop below, the true index of these candles is shifted \n",
    "                #down by one. The 'compensation' variable keeps track of this, and acouunts for it during the filtering process.\n",
    "                compensation = 0\n",
    "\n",
    "                #Tracks the progress of the loop\n",
    "                progress = 0.0\n",
    "\n",
    "                #Iterate through every index of the 'indeces_to_delete' list\n",
    "                for i in indeces_to_delete:\n",
    "\n",
    "                    #Prints the progress to the user\n",
    "                    '''if(progress % 30000 == 0):\n",
    "                        print(progress / float(len(indeces_to_delete)))\n",
    "\n",
    "                    progress += 1'''\n",
    "\n",
    "                    #Deletes the current loop iteration's respective index for every feature within\n",
    "                    #The list of features 'df1_data_list'\n",
    "                    for feature in range(len(features_list)):\n",
    "                        del df1_data_list[feature][i - compensation]\n",
    "\n",
    "                    #Increases 'compensation' by one to account for the shift in data caused by index deletion\n",
    "                    compensation += 1\n",
    "\n",
    "            #Status update\n",
    "            '''print(f'Exporting {ticker} minute data')'''\n",
    "\n",
    "            #Creates the DataFrame containing the minute data and writes it to a CSV File\n",
    "            minute_data_df = pd.DataFrame()\n",
    "\n",
    "            minute_data_df[f'{ticker}_Volume'] = df1_data_list[features_list.index(f'{ticker}_Volume')]\n",
    "            minute_data_df[f'{ticker}_High'] = df1_data_list[features_list.index(f'{ticker}_High')]\n",
    "            minute_data_df[f'{ticker}_Low'] = df1_data_list[features_list.index(f'{ticker}_Low')]\n",
    "            minute_data_df[f'{ticker}_Open'] = df1_data_list[features_list.index(f'{ticker}_Open')]\n",
    "            minute_data_df[f'{ticker}_Close'] = df1_data_list[features_list.index(f'{ticker}_Close')]\n",
    "            minute_data_df['Date'] = df1_data_list[features_list.index('Date')]\n",
    "\n",
    "            minute_data_df.drop_duplicates(inplace = True)\n",
    "\n",
    "            #minute_data_df.to_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "\n",
    "            #Status update\n",
    "            '''print(f'Exporting {ticker} Daily data')'''\n",
    "            \n",
    "        #If the 'daily' parameter is set to True\n",
    "        if include_daily:\n",
    "        \n",
    "            #Beginning of Daily Data collection\n",
    "            daily_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "\n",
    "            daily_data_df = requests.get(daily_url).json()\n",
    "            daily_data_df = pd.DataFrame(daily_data_df['results'])\n",
    "\n",
    "            daily_data_df.reset_index(inplace = True)\n",
    "            daily_data_df.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "\n",
    "            daily_data_df['t'] = pd.to_datetime(daily_data_df['t'], unit = 'ms')\n",
    "            daily_data_df.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                 'o': f'{ticker}_Open', \n",
    "                                 'h': f'{ticker}_High',\n",
    "                                 'l': f'{ticker}_Low',\n",
    "                                 'v': f'{ticker}_Volume',\n",
    "                                 't': 'Date'}, inplace = True)\n",
    "\n",
    "            #daily_data.to_csv(f'DAILY_STOCK_DATA//{ticker}.csv')\n",
    "            \n",
    "        #Returns the proper DateFrames depending on which (or both) of the 'minute'/'daily' parameters are True\n",
    "        if include_minute and include_daily:\n",
    "            return minute_data_df, daily_data_df\n",
    "        elif include_minute:\n",
    "            return minute_data_df\n",
    "        elif include_daily:\n",
    "            return daily_data_df\n",
    "        #If neither the 'minute' nor the 'daily' parameter is True, prints an error statement\n",
    "        else:\n",
    "            print(\"No data type specified\")\n",
    "        '''print(f'Done with {ticker}')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14ef309-760a-4a66-a65b-bc248468e155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calls the 'get_ticker_data' function for every ticker in the SP500 (excluding some oddities)\n",
    "def acquire_all_data(start, end):\n",
    "    \n",
    "    for ticker in range(0, len(tickers)):\n",
    "        try:\n",
    "            mdf, ddf = get_ticker_data(tickers[ticker], start, end)\n",
    "            mdf.to_csv(f'MINUTE_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "            ddf.to_csv(f'DAILY_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "        except:\n",
    "            print(\"Too may calls, delaying for thirteen seconds and retrying\")\n",
    "            time.sleep(13)\n",
    "            try:\n",
    "                mdf, ddf = get_ticker_data(tickers[ticker], start, end)\n",
    "                mdf.to_csv(f'MINUTE_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "                ddf.to_csv(f'DAILY_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "            except:\n",
    "                print(f\"Unable to acquire ticker {tickers[ticker]} daily/minute data\")\n",
    "            continue\n",
    "            \n",
    "    #Gets minute data including after hours\n",
    "    for ticker in range(0, len(tickers)):\n",
    "        try:\n",
    "            mdfaf= get_ticker_data(tickers[ticker], start, end, include_daily = False, ignore_after_hours = False)\n",
    "            mdfaf.to_csv(f'MINUTE_STOCK_DATA_AFTERHOURS//{tickers[ticker]}.csv')\n",
    "        except:\n",
    "            print(\"Too may calls, delaying for thirteen seconds and retrying\")\n",
    "            time.sleep(13)\n",
    "            try:\n",
    "                mdfaf = get_ticker_data(tickers[ticker], start, end, include_daily = False, ignore_after_hours = False)\n",
    "                mdfaf.to_csv(f'MINUTE_STOCK_DATA_AFTERHOURS//{tickers[ticker]}.csv')\n",
    "            except:\n",
    "                print(f\"Unable to acquire ticker {tickers[ticker]} after hours data\")\n",
    "            continue\n",
    "    print('Complete')\n",
    "\n",
    "#acquire_all_data(start = '2017-07-08', end = '2022-07-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e6eb67-62ff-410f-8fbb-1eb1f3411f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ticker_data(ticker, end_date, ignore_after_hours = True, include_minute = True, include_daily = True):\n",
    "    if not include_minute and not include_daily:\n",
    "        print(\"No time window specified\")\n",
    "        \n",
    "    if include_minute:\n",
    "        df1 = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "        df1.drop(['Unnamed: 0'], inplace = True, axis = 1)\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for col in df1.columns:\n",
    "            features_list.append(col)\n",
    "        \n",
    "        start_date = df1['Date'].to_list()[-1][:10]\n",
    "        end = end_date\n",
    "        \n",
    "        sort = 'asc'\n",
    "        \n",
    "        minute_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{start_date}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "  \n",
    "        #Requests the minute stock data from the url listed above and creates the proper dataframe    \n",
    "        minute_data = requests.get(minute_url).json()\n",
    "        df2 = pd.DataFrame(minute_data['results'])\n",
    "        df2.reset_index(inplace = True)\n",
    "        df2.drop(['vw', 'n', 'index', 'a', 'op'], axis = 1, inplace = True)\n",
    "\n",
    "        df2['t'] = pd.to_datetime(df2['t'], unit = 'ms')\n",
    "        df2.rename(columns = {'c': f'{ticker}_Close', \n",
    "                             'o': f'{ticker}_Open', \n",
    "                             'h': f'{ticker}_High',\n",
    "                             'l': f'{ticker}_Low',\n",
    "                             'v': f'{ticker}_Volume',\n",
    "                             't': 'Date'}, inplace = True)\n",
    "        \n",
    "        features_list2 = []\n",
    "        \n",
    "        for col in df2.columns:\n",
    "            features_list2.append(col)\n",
    "      \n",
    "        df1_data_list = []\n",
    "        \n",
    "        for col in df1.columns:\n",
    "            df1_data_list.append(df1[col].to_list())\n",
    "                \n",
    "        df2_data_list = []\n",
    "        \n",
    "        for col in df2.columns:\n",
    "            df2_data_list.append(df2[col].to_list())\n",
    "            \n",
    "        if ignore_after_hours:\n",
    "\n",
    "            #List containing all of the indeces that are to be deleted\n",
    "            indeces_to_delete = []\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Finding all pre/post market data\")'''\n",
    "            \n",
    "            progress = 0.0\n",
    "\n",
    "            #This loop will iterate through every data point in the data and determine the indeces of all \n",
    "            #pre/post market data\n",
    "            for i in range(len(df2_data_list[features_list.index('Date')])):\n",
    "\n",
    "                #Prints the progress of this loop\n",
    "                '''if(progress % 50000 == 0):\n",
    "                    print(progress / float(len(df1_data_list[features_list.index('Date')])))'''\n",
    "\n",
    "                progress += 1\n",
    "\n",
    "                #Sets the hour/minute values of each iteration\n",
    "                hour = int(str(df2_data_list[features_list.index('Date')][i])[11:13])\n",
    "                minute = int(str(df2_data_list[features_list.index('Date')][i])[14:16])\n",
    "\n",
    "                #Appends the index of all of the candles that occur before 1:30 pm and after\n",
    "                #8:00 pm (since those are the beginning and end times for the recorded stock data\n",
    "\n",
    "                if hour < 13:\n",
    "                    indeces_to_delete.append(i)\n",
    "                elif hour >= 20:\n",
    "                    indeces_to_delete.append(i)\n",
    "                elif hour == 13 and minute < 30:\n",
    "                    indeces_to_delete.append(i)\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Deleting all pre/post market data\")'''\n",
    "\n",
    "            #This variable accounts for the shift in the data that occurs every time an index is deleted.\n",
    "            #The 'indeces_to_delete' list contains all of the indeces of candles that occur during pre/post market,\n",
    "            #but every time one of those is deleted in the loop below, the true index of these candles is shifted \n",
    "            #down by one. The 'compensation' variable keeps track of this, and acouunts for it during the filtering process.\n",
    "            compensation = 0\n",
    "\n",
    "            #Tracks the progress of the loop\n",
    "            progress = 0.0\n",
    "\n",
    "            #Iterate through every index of the 'indeces_to_delete' list\n",
    "            for i in indeces_to_delete:\n",
    "\n",
    "                #Prints the progress to the user\n",
    "                '''if(progress % 30000 == 0):\n",
    "                    print(progress / float(len(indeces_to_delete)))'''\n",
    "\n",
    "                progress += 1\n",
    "\n",
    "                #Deletes the current loop iteration's respective index for every feature within\n",
    "                #The list of features 'df1_data_list'\n",
    "                for feature in range(len(features_list)):\n",
    "                    del df2_data_list[feature][i - compensation]\n",
    "\n",
    "                #Increases 'compensation' by one to account for the shift in data caused by index deletion\n",
    "                compensation += 1\n",
    "\n",
    "        for i in range(len(df2_data_list[features_list2.index('Date')])):\n",
    "            df2_data_list[features_list2.index('Date')][i] = str(df2_data_list[features_list2.index('Date')][i])\n",
    "            \n",
    "        for feature in features_list:\n",
    "            df1_data_list[features_list.index(feature)].extend(df2_data_list[features_list2.index(feature)])\n",
    "            \n",
    "        minute_df = pd.DataFrame()\n",
    "        \n",
    "        minute_df[f'{ticker}_High'] = df1_data_list[features_list.index(f'{ticker}_High')]\n",
    "        minute_df[f'{ticker}_Low'] = df1_data_list[features_list.index(f'{ticker}_Low')]\n",
    "        minute_df[f'{ticker}_Open'] = df1_data_list[features_list.index(f'{ticker}_Open')]\n",
    "        minute_df[f'{ticker}_Close'] = df1_data_list[features_list.index(f'{ticker}_Close')]\n",
    "        minute_df[f'{ticker}_Volume'] = df1_data_list[features_list.index(f'{ticker}_Volume')]\n",
    "        minute_df['Date'] = df1_data_list[features_list.index('Date')]\n",
    "       \n",
    "        minute_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    print(f'Updating {ticker}')\n",
    " \n",
    "    if include_minute and include_daily:\n",
    "        return minute_df#, daily_df\n",
    "    elif include_minute:\n",
    "        return minute_df\n",
    "    elif include_daily:\n",
    "        return daily_df\n",
    "    \n",
    "#tst = update_ticker_data('AAPL', '2022-07-22')\n",
    "#tst.to_csv('MINUTE_STOCK_DATA//AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676e8568-b1bc-4ec4-bc1e-4a69366b68a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_all_ticker_data(end):\n",
    "    for ticker in range(0, len(tickers)):\n",
    "        try:\n",
    "            mdf, ddf = update_ticker_data(tickers[ticker], end_date = end)\n",
    "            mdf.to_csv(f'MINUTE_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "            ddf.to_csv(f'DAILY_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "        except:\n",
    "            print(\"Too may calls, delaying for thirteen seconds and retrying\")\n",
    "            time.sleep(13)\n",
    "            try:\n",
    "                mdf, ddf = get_ticker_data(tickers[ticker], start, end)\n",
    "                mdf.to_csv(f'MINUTE_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "                ddf.to_csv(f'DAILY_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "            except:\n",
    "                print(f\"Unable to update ticker {tickers[ticker]} daily/minute data\")\n",
    "            continue\n",
    "    \n",
    "    #Updates after hours minute data\n",
    "    for ticker in range(0, len(tickers)):\n",
    "        try:\n",
    "            mdfaf = update_ticker_data(tickers[ticker], end_date = end, include_daily = False, ignore_after_hours = False)\n",
    "            mdfaf.to_csv(f'MINUTE_STOCK_DATA_AFTERHOURS//{tickers[ticker]}.csv')\n",
    "        except:\n",
    "            print(\"Too may calls, delaying for thirteen seconds and retrying\")\n",
    "            time.sleep(13)\n",
    "            try:\n",
    "                mdfaf = get_ticker_data(tickers[ticker], start, end, include_daily = False, ignore_after_hours = False)\n",
    "                mdfaf.to_csv(f'MINUTE_STOCK_DATA_AFTERHOURS//{tickers[ticker]}.csv')\n",
    "            except:\n",
    "                print(f\"Unable to update ticker {tickers[ticker]} after hours data\")\n",
    "            continue\n",
    "    print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35a2d26-6442-4b16-95f4-2001d2901a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method processes the minute data for Deep Learning. It adds a 'target_column' which signifies whether the \n",
    "#neural network should predict a long or a short position for a stoc. \n",
    "def process_minute_data(ticker):\n",
    "    \n",
    "    #Loads the minute data from csv file\n",
    "    df = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "    \n",
    "    #Drops the 'Date' and the other extraneous column (idk how it got there I didn't make it), since they will not be needed\n",
    "    df.drop(['Date', 'Unnamed: 0'], axis = 1, inplace = True)\n",
    "    \n",
    "    #This list will contain lists of the future values for a specific candle. Each list within 'future_column' will\n",
    "    #be 'hm_units' long. This list is never put into the data frame that will be fed through the neural network. It is\n",
    "    #created for the soul purpose of creating the 'target_column' which tells the Neural network whether or not its guess\n",
    "    #was correct. Each list of futures within this list will befed through the 'classify' method, along with the close \n",
    "    #price of the candle that directly preceeds the first index of the list. Again, this is only a temporary list, and \n",
    "    #will be discarded after the function has completed its process. The 'future_column' list will also be 'hm_units'\n",
    "    #shorter than the final data frame bc the 'classify' function can't classify values that run outside of the index\n",
    "    #of the close prices. (ie, if the 'classify' function must create a result 'hm_units' into the future, and it \n",
    "    #is fed the very last data point of the close prices, there is no future data to calculate %discrepency against.\n",
    "    future_lows_column = []\n",
    "    \n",
    "    #This loop appends a list of 'hm_units' future values to the 'future_column' list\n",
    "    for i in range(len(df) - hm_units):\n",
    "        \n",
    "        \n",
    "        future_lows = []\n",
    "        \n",
    "        #Appends each future value to the 'futures' list, skipping the first (since that is the current value)\n",
    "        for ii in range(1, hm_units + 1):\n",
    "            future_lows.append(df[f'{ticker}_Low'].iloc[i + ii])\n",
    "    \n",
    "        future_lows_column.append(future_lows)\n",
    "    \n",
    "    #Target column which will be added to the data frame. This column essentially uses the 'classify' function\n",
    "    #to calculate whether a stock will significantly increase or decrease during the next 'hm_units' minute candles.\n",
    "    target_column = []\n",
    "    \n",
    "    #Feeds every close value of the current ticker and every list within the 'future_column' list through the 'classify'\n",
    "    #function, and appends it to the 'target_column'\n",
    "    for i in range(len(future_lows_column)):\n",
    "        target_column.append(classify(df[f'{ticker}_Close'].iloc[i], df[f'{ticker}_Close'].iloc[i + hm_units], future_lows_column[i]))\n",
    "    \n",
    "    #Since the 'target_column' can't make predictions with data it doesn't have (when reaching the end of the close prices,\n",
    "    #The last 'hm_units' of the 'target_column' is simply filled with a zero. Bc each dataframe has roughly 150k to 200k \n",
    "    #datapoints, these zeros should not affect accuracy in the slightest\n",
    "    for i in range(hm_units):\n",
    "        target_column.append(0)\n",
    "        \n",
    "    \n",
    "    #Adds the minute atr column to the data frame\n",
    "    df[f'{ticker}_Minute_ATR'] = Minute_ATR_Column(ticker)\n",
    "    \n",
    "    #Adds a 9, 50, 100, and 200 MA to the model\n",
    "    df[f'{ticker}_9_MA'], df[f'{ticker}_50_MA'], df[f'{ticker}_100_MA'], df[f'{ticker}_200_MA'] = moving_averages_column(ticker, minute = True)\n",
    "    \n",
    "    #Adds the 'target_column' list to the dataframe, and returns the dataframe\n",
    "    df[f'{ticker}_target'] = target_column\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "874ea09d-58b3-4dd3-aa05-e1710e946a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits sequential data into a training and testing set\n",
    "def split_sequential_data(seq_data):\n",
    "    \n",
    "    last_20pct = -int(len(seq_data) * 0.20)\n",
    "    \n",
    "    training_data = seq_data[:last_20pct]\n",
    "    backtesting_data = seq_data[last_20pct:]\n",
    "    \n",
    "    return training_data, backtesting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9efb7342-c8f5-46f0-8ab7-f8b1d76b6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_minute_for_DL(ticker):\n",
    "    \n",
    "    df = process_minute_data(ticker)\n",
    "    \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #Scales/Normalizes data\n",
    "    for col in df.columns:\n",
    "        #if col != f'{ticker}_target' or col != f'{ticker}_RSI_Weights':\n",
    "        if col != f'{ticker}_target':\n",
    "            df[col] = df[col].pct_change()\n",
    "        \n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_units = deque(maxlen = SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_units.append([n for n in i[:-1]])\n",
    "        if len(prev_units) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_units), i[-1]])\n",
    "            \n",
    "    print(\"Number of total data points:\")\n",
    "    print(len(sequential_data))\n",
    "    \n",
    "    training_sequential, testing_sequential = split_sequential_data(sequential_data)\n",
    "    \n",
    "    random.shuffle(training_sequential)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    " \n",
    "    for seq, target in training_sequential:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])    \n",
    "   \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    print(\"Num buys/sells:\")\n",
    "    print(len(buys))\n",
    "    print(len(sells))\n",
    "\n",
    "    training_sequential = buys + sells\n",
    "    random.shuffle(training_sequential)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in training_sequential:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y), testing_sequential\n",
    "\n",
    "#X, y = preprocess_df_minute('AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6573e65-301e-4e96-9b61-b2d3341edc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dl(ticker):\n",
    "    X, y, back_testing_data = reformat_minute_for_DL(ticker)\n",
    "    \n",
    "    last_5pct = -int(len(X) * 0.05)\n",
    "    \n",
    "    X_train = X[:last_5pct]\n",
    "    y_train = y[:last_5pct]\n",
    "    X_test = X[last_5pct:]\n",
    "    y_test = y[last_5pct:]\n",
    "    \n",
    "    print(\"Number of training data points:\")\n",
    "    print(len(X_train))\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape = (X_train.shape[1:]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(64, input_shape = (X_train.shape[1:]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(64, input_shape = (X_train.shape[1:])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "              \n",
    "    model.add(Dense(256, activation = 'relu')) #512\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001, decay = 1e-6)\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt, metrics = ['accuracy']) \n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir = f'Records/logs/{NAME}')\n",
    "    \n",
    "    filepath = 'RNN_Final-{epoch: 02d}-{val_accuracy: .3f}'\n",
    "    checkpoint = ModelCheckpoint('Records/models/{}.model'.format(filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max'))\n",
    "  \n",
    "    history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_test, y_test), callbacks = [tensorboard, checkpoint]) \n",
    "    \n",
    "    return back_testing_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f056633c-4a2c-43dc-bf13-621f48b89491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Back testing\n",
    "\n",
    "def back_test(data, model_location, test_all = False, test_random_week = True):\n",
    "    \n",
    "    print(\"Attempting model back test\")\n",
    "    \n",
    "    if not test_random_week and not test_all:\n",
    "        print(\"No testing method specified\")\n",
    "        \n",
    "    if test_random_week:\n",
    "        \n",
    "        model = tf.keras.models.load_model(model_location)\n",
    "\n",
    "        index = random.randrange(len(data) - 390)\n",
    "        \n",
    "        print(f\"Testing day: {index}\")\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(390):\n",
    "            X.append(data[index + i][0])\n",
    "            y.append(data[index + i][1])\n",
    " \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        prediction_probabilities = model.predict(X)\n",
    "        predictions = np.argmax(prediction_probabilities, axis = 1)\n",
    "\n",
    "        total_count = 0.0\n",
    "        count0 = 0.0\n",
    "        count1 = 0.0\n",
    "\n",
    "        print(\"Total accuracy:\")\n",
    "        for i in range(len(y)):\n",
    "            if(predictions[i] == y[i]):\n",
    "                total_count += 1.0\n",
    "\n",
    "        print(total_count / float(len(y)))\n",
    "        \n",
    "        indeces_0 = []\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 0:\n",
    "                indeces_0.append(i)\n",
    "\n",
    "        print(\"Accuracy of '0' predictions:\")\n",
    "        for i in indeces_0:\n",
    "            if(predictions[i] == 0):\n",
    "                count0 += 1.0\n",
    "\n",
    "        print(count0 / float(len(indeces_0)))\n",
    "        \n",
    "        indeces_1 = []\n",
    "\n",
    "        print(\"Accuracy of '1' predictions:\")\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 1:\n",
    "                indeces_1.append(i)\n",
    "\n",
    "        for i in indeces_1:\n",
    "            if(predictions[i] == 1):\n",
    "                count1 += 1.0\n",
    "\n",
    "        print(count1 / float(len(indeces_1)))\n",
    "    \n",
    "    print(\"number of true buys/sells\")\n",
    "    print(len(indeces_1), len(indeces_0))\n",
    "    \n",
    "    return count1 / float(len(indeces_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d471258f-5d54-49d0-88fb-a267e357cd67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total data points:\n",
      "487236\n",
      "Num buys/sells:\n",
      "49979\n",
      "49979\n",
      "Number of training data points:\n",
      "94961\n",
      "Epoch 1/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.6739 - accuracy: 0.5942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 1- 0.570.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 1- 0.570.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 29s 34ms/step - loss: 0.6739 - accuracy: 0.5941 - val_loss: 0.6777 - val_accuracy: 0.5697\n",
      "Epoch 2/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.6506 - accuracy: 0.6224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 2- 0.639.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 2- 0.639.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 26s 35ms/step - loss: 0.6505 - accuracy: 0.6226 - val_loss: 0.6436 - val_accuracy: 0.6388\n",
      "Epoch 3/20\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.6343"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 3- 0.650.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 3- 0.650.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 26s 35ms/step - loss: 0.6422 - accuracy: 0.6343 - val_loss: 0.6275 - val_accuracy: 0.6498\n",
      "Epoch 4/20\n",
      "740/742 [============================>.] - ETA: 0s - loss: 0.6367 - accuracy: 0.6397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 4- 0.517.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 4- 0.517.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 25s 34ms/step - loss: 0.6367 - accuracy: 0.6397 - val_loss: 0.7938 - val_accuracy: 0.5173\n",
      "Epoch 5/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.6327 - accuracy: 0.6461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 5- 0.653.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 5- 0.653.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 28s 38ms/step - loss: 0.6327 - accuracy: 0.6461 - val_loss: 0.6271 - val_accuracy: 0.6534\n",
      "Epoch 6/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.6292 - accuracy: 0.6478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 6- 0.641.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 6- 0.641.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 31s 42ms/step - loss: 0.6292 - accuracy: 0.6478 - val_loss: 0.6367 - val_accuracy: 0.6410\n",
      "Epoch 7/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.6232 - accuracy: 0.6552"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 7- 0.667.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 7- 0.667.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 27s 36ms/step - loss: 0.6232 - accuracy: 0.6553 - val_loss: 0.6227 - val_accuracy: 0.6668\n",
      "Epoch 8/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.6065 - accuracy: 0.6755"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 8- 0.569.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 8- 0.569.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 28s 38ms/step - loss: 0.6065 - accuracy: 0.6755 - val_loss: 0.6694 - val_accuracy: 0.5693\n",
      "Epoch 9/20\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.5935 - accuracy: 0.6881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 9- 0.516.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 9- 0.516.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 32s 43ms/step - loss: 0.5935 - accuracy: 0.6881 - val_loss: 0.7768 - val_accuracy: 0.5155\n",
      "Epoch 10/20\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.6937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 10- 0.630.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 10- 0.630.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 32s 43ms/step - loss: 0.5851 - accuracy: 0.6937 - val_loss: 0.6446 - val_accuracy: 0.6304\n",
      "Epoch 11/20\n",
      "740/742 [============================>.] - ETA: 0s - loss: 0.5789 - accuracy: 0.6982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 11- 0.646.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 11- 0.646.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 29s 39ms/step - loss: 0.5789 - accuracy: 0.6983 - val_loss: 0.6272 - val_accuracy: 0.6462\n",
      "Epoch 12/20\n",
      "742/742 [==============================] - ETA: 0s - loss: 0.5706 - accuracy: 0.7040"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 12- 0.657.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 12- 0.657.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 27s 36ms/step - loss: 0.5706 - accuracy: 0.7040 - val_loss: 0.6204 - val_accuracy: 0.6568\n",
      "Epoch 13/20\n",
      "740/742 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 13- 0.719.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 13- 0.719.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 28s 38ms/step - loss: 0.5632 - accuracy: 0.7091 - val_loss: 0.5616 - val_accuracy: 0.7190\n",
      "Epoch 14/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.5556 - accuracy: 0.7145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 14- 0.698.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 14- 0.698.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 28s 38ms/step - loss: 0.5556 - accuracy: 0.7145 - val_loss: 0.5776 - val_accuracy: 0.6980\n",
      "Epoch 15/20\n",
      "740/742 [============================>.] - ETA: 0s - loss: 0.5477 - accuracy: 0.7204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 15- 0.684.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 15- 0.684.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 24s 33ms/step - loss: 0.5477 - accuracy: 0.7204 - val_loss: 0.5917 - val_accuracy: 0.6836\n",
      "Epoch 16/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.5397 - accuracy: 0.7262"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 16- 0.695.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 16- 0.695.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 23s 31ms/step - loss: 0.5398 - accuracy: 0.7261 - val_loss: 0.5947 - val_accuracy: 0.6946\n",
      "Epoch 17/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.5294 - accuracy: 0.7334"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 17- 0.685.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 17- 0.685.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 26s 35ms/step - loss: 0.5294 - accuracy: 0.7335 - val_loss: 0.5897 - val_accuracy: 0.6854\n",
      "Epoch 18/20\n",
      "740/742 [============================>.] - ETA: 0s - loss: 0.5203 - accuracy: 0.7412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 18- 0.694.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 18- 0.694.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 25s 33ms/step - loss: 0.5203 - accuracy: 0.7411 - val_loss: 0.6032 - val_accuracy: 0.6936\n",
      "Epoch 19/20\n",
      "740/742 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.7475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 19- 0.730.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 19- 0.730.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 25s 34ms/step - loss: 0.5111 - accuracy: 0.7475 - val_loss: 0.5397 - val_accuracy: 0.7298\n",
      "Epoch 20/20\n",
      "741/742 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.7539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4053_layer_call_fn, lstm_cell_4053_layer_call_and_return_conditional_losses, lstm_cell_4054_layer_call_fn, lstm_cell_4054_layer_call_and_return_conditional_losses, lstm_cell_4055_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 20- 0.746.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Records/models\\RNN_Final- 20- 0.746.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742/742 [==============================] - 27s 36ms/step - loss: 0.5015 - accuracy: 0.7538 - val_loss: 0.5124 - val_accuracy: 0.7464\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 #128 - original --- 32 = optimized (potentially)\n",
    "LSIZE = 128 #128 - original --- 64 = optimized (porentially)\n",
    "EPOCHS = 20\n",
    "hm_units = 10\n",
    "req = .002\n",
    "\n",
    "back_test_data = do_dl('AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ff08b246-f700-4c84-b9da-fd91083a183e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting model back test\n",
      "Testing day: 86208\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.41794871794871796\n",
      "Accuracy of '0' predictions:\n",
      "0.33974358974358976\n",
      "Accuracy of '1' predictions:\n",
      "0.7307692307692307\n",
      "number of true buys/sells\n",
      "78 312\n",
      "Attempting model back test\n",
      "Testing day: 67061\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.41025641025641024\n",
      "Accuracy of '0' predictions:\n",
      "0.38461538461538464\n",
      "Accuracy of '1' predictions:\n",
      "0.6410256410256411\n",
      "number of true buys/sells\n",
      "39 351\n",
      "Attempting model back test\n",
      "Testing day: 95339\n",
      "13/13 [==============================] - 1s 25ms/step\n",
      "Total accuracy:\n",
      "0.6948717948717948\n",
      "Accuracy of '0' predictions:\n",
      "0.7376093294460642\n",
      "Accuracy of '1' predictions:\n",
      "0.3829787234042553\n",
      "number of true buys/sells\n",
      "47 343\n",
      "Attempting model back test\n",
      "Testing day: 61330\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.3923076923076923\n",
      "Accuracy of '0' predictions:\n",
      "0.30194805194805197\n",
      "Accuracy of '1' predictions:\n",
      "0.7317073170731707\n",
      "number of true buys/sells\n",
      "82 308\n",
      "Attempting model back test\n",
      "Testing day: 85916\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.49230769230769234\n",
      "Accuracy of '0' predictions:\n",
      "0.4720670391061452\n",
      "Accuracy of '1' predictions:\n",
      "0.71875\n",
      "number of true buys/sells\n",
      "32 358\n",
      "Attempting model back test\n",
      "Testing day: 85860\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.5153846153846153\n",
      "Accuracy of '0' predictions:\n",
      "0.5136612021857924\n",
      "Accuracy of '1' predictions:\n",
      "0.5416666666666666\n",
      "number of true buys/sells\n",
      "24 366\n",
      "Attempting model back test\n",
      "Testing day: 54969\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.6435897435897436\n",
      "Accuracy of '0' predictions:\n",
      "0.6505681818181818\n",
      "Accuracy of '1' predictions:\n",
      "0.5789473684210527\n",
      "number of true buys/sells\n",
      "38 352\n",
      "Attempting model back test\n",
      "Testing day: 11345\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.6410256410256411\n",
      "Accuracy of '0' predictions:\n",
      "0.662613981762918\n",
      "Accuracy of '1' predictions:\n",
      "0.5245901639344263\n",
      "number of true buys/sells\n",
      "61 329\n",
      "Attempting model back test\n",
      "Testing day: 69339\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.5025641025641026\n",
      "Accuracy of '0' predictions:\n",
      "0.5191740412979351\n",
      "Accuracy of '1' predictions:\n",
      "0.39215686274509803\n",
      "number of true buys/sells\n",
      "51 339\n",
      "Attempting model back test\n",
      "Testing day: 74781\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.2564102564102564\n",
      "Accuracy of '0' predictions:\n",
      "0.14242424242424243\n",
      "Accuracy of '1' predictions:\n",
      "0.8833333333333333\n",
      "number of true buys/sells\n",
      "60 330\n",
      "Attempting model back test\n",
      "Testing day: 75377\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.382051282051282\n",
      "Accuracy of '0' predictions:\n",
      "0.11851851851851852\n",
      "Accuracy of '1' predictions:\n",
      "0.975\n",
      "number of true buys/sells\n",
      "120 270\n",
      "Attempting model back test\n",
      "Testing day: 66316\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.4564102564102564\n",
      "Accuracy of '0' predictions:\n",
      "0.46546546546546547\n",
      "Accuracy of '1' predictions:\n",
      "0.40350877192982454\n",
      "number of true buys/sells\n",
      "57 333\n",
      "Attempting model back test\n",
      "Testing day: 31402\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.5820512820512821\n",
      "Accuracy of '0' predictions:\n",
      "0.608433734939759\n",
      "Accuracy of '1' predictions:\n",
      "0.43103448275862066\n",
      "number of true buys/sells\n",
      "58 332\n",
      "Attempting model back test\n",
      "Testing day: 44512\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.36153846153846153\n",
      "Accuracy of '0' predictions:\n",
      "0.29012345679012347\n",
      "Accuracy of '1' predictions:\n",
      "0.7121212121212122\n",
      "number of true buys/sells\n",
      "66 324\n",
      "Attempting model back test\n",
      "Testing day: 80663\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.32051282051282054\n",
      "Accuracy of '0' predictions:\n",
      "0.02214022140221402\n",
      "Accuracy of '1' predictions:\n",
      "1.0\n",
      "number of true buys/sells\n",
      "119 271\n",
      "Attempting model back test\n",
      "Testing day: 48984\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.39487179487179486\n",
      "Accuracy of '0' predictions:\n",
      "0.2037735849056604\n",
      "Accuracy of '1' predictions:\n",
      "0.8\n",
      "number of true buys/sells\n",
      "125 265\n",
      "Attempting model back test\n",
      "Testing day: 61335\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.3923076923076923\n",
      "Accuracy of '0' predictions:\n",
      "0.30194805194805197\n",
      "Accuracy of '1' predictions:\n",
      "0.7317073170731707\n",
      "number of true buys/sells\n",
      "82 308\n",
      "Attempting model back test\n",
      "Testing day: 90108\n",
      "13/13 [==============================] - 2s 22ms/step\n",
      "Total accuracy:\n",
      "0.5717948717948718\n",
      "Accuracy of '0' predictions:\n",
      "0.5316091954022989\n",
      "Accuracy of '1' predictions:\n",
      "0.9047619047619048\n",
      "number of true buys/sells\n",
      "42 348\n",
      "Attempting model back test\n",
      "Testing day: 15633\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.617948717948718\n",
      "Accuracy of '0' predictions:\n",
      "0.5936599423631124\n",
      "Accuracy of '1' predictions:\n",
      "0.813953488372093\n",
      "number of true buys/sells\n",
      "43 347\n",
      "Attempting model back test\n",
      "Testing day: 10076\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.6974358974358974\n",
      "Accuracy of '0' predictions:\n",
      "0.6887052341597796\n",
      "Accuracy of '1' predictions:\n",
      "0.8148148148148148\n",
      "number of true buys/sells\n",
      "27 363\n",
      "Attempting model back test\n",
      "Testing day: 76975\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.2512820512820513\n",
      "Accuracy of '0' predictions:\n",
      "0.02666666666666667\n",
      "Accuracy of '1' predictions:\n",
      "1.0\n",
      "number of true buys/sells\n",
      "90 300\n",
      "Attempting model back test\n",
      "Testing day: 25143\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.7230769230769231\n",
      "Accuracy of '0' predictions:\n",
      "0.7214484679665738\n",
      "Accuracy of '1' predictions:\n",
      "0.7419354838709677\n",
      "number of true buys/sells\n",
      "31 359\n",
      "Attempting model back test\n",
      "Testing day: 79953\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.2794871794871795\n",
      "Accuracy of '0' predictions:\n",
      "0.15915915915915915\n",
      "Accuracy of '1' predictions:\n",
      "0.9824561403508771\n",
      "number of true buys/sells\n",
      "57 333\n",
      "Attempting model back test\n",
      "Testing day: 75088\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.17435897435897435\n",
      "Accuracy of '0' predictions:\n",
      "0.03592814371257485\n",
      "Accuracy of '1' predictions:\n",
      "1.0\n",
      "number of true buys/sells\n",
      "56 334\n",
      "Attempting model back test\n",
      "Testing day: 46002\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.5564102564102564\n",
      "Accuracy of '0' predictions:\n",
      "0.5485714285714286\n",
      "Accuracy of '1' predictions:\n",
      "0.625\n",
      "number of true buys/sells\n",
      "40 350\n",
      "Attempting model back test\n",
      "Testing day: 70894\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.4948717948717949\n",
      "Accuracy of '0' predictions:\n",
      "0.5027027027027027\n",
      "Accuracy of '1' predictions:\n",
      "0.35\n",
      "number of true buys/sells\n",
      "20 370\n",
      "Attempting model back test\n",
      "Testing day: 15908\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.6128205128205129\n",
      "Accuracy of '0' predictions:\n",
      "0.5957446808510638\n",
      "Accuracy of '1' predictions:\n",
      "0.7049180327868853\n",
      "number of true buys/sells\n",
      "61 329\n",
      "Attempting model back test\n",
      "Testing day: 54770\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.5512820512820513\n",
      "Accuracy of '0' predictions:\n",
      "0.5415472779369628\n",
      "Accuracy of '1' predictions:\n",
      "0.6341463414634146\n",
      "number of true buys/sells\n",
      "41 349\n",
      "Attempting model back test\n",
      "Testing day: 44116\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.4564102564102564\n",
      "Accuracy of '0' predictions:\n",
      "0.4447592067988669\n",
      "Accuracy of '1' predictions:\n",
      "0.5675675675675675\n",
      "number of true buys/sells\n",
      "37 353\n",
      "Attempting model back test\n",
      "Testing day: 46480\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.43846153846153846\n",
      "Accuracy of '0' predictions:\n",
      "0.4171597633136095\n",
      "Accuracy of '1' predictions:\n",
      "0.5769230769230769\n",
      "number of true buys/sells\n",
      "52 338\n",
      "Attempting model back test\n",
      "Testing day: 3875\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.7512820512820513\n",
      "Accuracy of '0' predictions:\n",
      "0.7479674796747967\n",
      "Accuracy of '1' predictions:\n",
      "0.8095238095238095\n",
      "number of true buys/sells\n",
      "21 369\n",
      "Attempting model back test\n",
      "Testing day: 72076\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.3487179487179487\n",
      "Accuracy of '0' predictions:\n",
      "0.3303303303303303\n",
      "Accuracy of '1' predictions:\n",
      "0.45614035087719296\n",
      "number of true buys/sells\n",
      "57 333\n",
      "Attempting model back test\n",
      "Testing day: 56607\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.5128205128205128\n",
      "Accuracy of '0' predictions:\n",
      "0.4556213017751479\n",
      "Accuracy of '1' predictions:\n",
      "0.8846153846153846\n",
      "number of true buys/sells\n",
      "52 338\n",
      "Attempting model back test\n",
      "Testing day: 57503\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.43333333333333335\n",
      "Accuracy of '0' predictions:\n",
      "0.16475095785440613\n",
      "Accuracy of '1' predictions:\n",
      "0.9767441860465116\n",
      "number of true buys/sells\n",
      "129 261\n",
      "Attempting model back test\n",
      "Testing day: 10749\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.7410256410256411\n",
      "Accuracy of '0' predictions:\n",
      "0.7774647887323943\n",
      "Accuracy of '1' predictions:\n",
      "0.37142857142857144\n",
      "number of true buys/sells\n",
      "35 355\n",
      "Attempting model back test\n",
      "Testing day: 28867\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.717948717948718\n",
      "Accuracy of '0' predictions:\n",
      "0.7214484679665738\n",
      "Accuracy of '1' predictions:\n",
      "0.6774193548387096\n",
      "number of true buys/sells\n",
      "31 359\n",
      "Attempting model back test\n",
      "Testing day: 75807\n",
      "13/13 [==============================] - 2s 22ms/step\n",
      "Total accuracy:\n",
      "0.46923076923076923\n",
      "Accuracy of '0' predictions:\n",
      "0.3745583038869258\n",
      "Accuracy of '1' predictions:\n",
      "0.719626168224299\n",
      "number of true buys/sells\n",
      "107 283\n",
      "Attempting model back test\n",
      "Testing day: 32587\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.49743589743589745\n",
      "Accuracy of '0' predictions:\n",
      "0.463855421686747\n",
      "Accuracy of '1' predictions:\n",
      "0.6896551724137931\n",
      "number of true buys/sells\n",
      "58 332\n",
      "Attempting model back test\n",
      "Testing day: 10309\n",
      "13/13 [==============================] - 3s 22ms/step\n",
      "Total accuracy:\n",
      "0.7256410256410256\n",
      "Accuracy of '0' predictions:\n",
      "0.7520661157024794\n",
      "Accuracy of '1' predictions:\n",
      "0.37037037037037035\n",
      "number of true buys/sells\n",
      "27 363\n",
      "Attempting model back test\n",
      "Testing day: 28414\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.541025641025641\n",
      "Accuracy of '0' predictions:\n",
      "0.5413333333333333\n",
      "Accuracy of '1' predictions:\n",
      "0.5333333333333333\n",
      "number of true buys/sells\n",
      "15 375\n",
      "Attempting model back test\n",
      "Testing day: 76677\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.3153846153846154\n",
      "Accuracy of '0' predictions:\n",
      "0.022058823529411766\n",
      "Accuracy of '1' predictions:\n",
      "0.9915254237288136\n",
      "number of true buys/sells\n",
      "118 272\n",
      "Attempting model back test\n",
      "Testing day: 36250\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.6282051282051282\n",
      "Accuracy of '0' predictions:\n",
      "0.6042944785276073\n",
      "Accuracy of '1' predictions:\n",
      "0.75\n",
      "number of true buys/sells\n",
      "64 326\n",
      "Attempting model back test\n",
      "Testing day: 75751\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.4205128205128205\n",
      "Accuracy of '0' predictions:\n",
      "0.27335640138408307\n",
      "Accuracy of '1' predictions:\n",
      "0.8415841584158416\n",
      "number of true buys/sells\n",
      "101 289\n",
      "Attempting model back test\n",
      "Testing day: 22132\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.8102564102564103\n",
      "Accuracy of '0' predictions:\n",
      "0.8189189189189189\n",
      "Accuracy of '1' predictions:\n",
      "0.65\n",
      "number of true buys/sells\n",
      "20 370\n",
      "Attempting model back test\n",
      "Testing day: 83492\n",
      "13/13 [==============================] - 1s 23ms/step\n",
      "Total accuracy:\n",
      "0.4\n",
      "Accuracy of '0' predictions:\n",
      "0.33876221498371334\n",
      "Accuracy of '1' predictions:\n",
      "0.6265060240963856\n",
      "number of true buys/sells\n",
      "83 307\n",
      "Attempting model back test\n",
      "Testing day: 39643\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.38974358974358975\n",
      "Accuracy of '0' predictions:\n",
      "0.22535211267605634\n",
      "Accuracy of '1' predictions:\n",
      "0.8301886792452831\n",
      "number of true buys/sells\n",
      "106 284\n",
      "Attempting model back test\n",
      "Testing day: 19779\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.7333333333333333\n",
      "Accuracy of '0' predictions:\n",
      "0.78419452887538\n",
      "Accuracy of '1' predictions:\n",
      "0.45901639344262296\n",
      "number of true buys/sells\n",
      "61 329\n",
      "Attempting model back test\n",
      "Testing day: 75560\n",
      "13/13 [==============================] - 1s 22ms/step\n",
      "Total accuracy:\n",
      "0.43333333333333335\n",
      "Accuracy of '0' predictions:\n",
      "0.1947565543071161\n",
      "Accuracy of '1' predictions:\n",
      "0.9512195121951219\n",
      "number of true buys/sells\n",
      "123 267\n",
      "Attempting model back test\n",
      "Testing day: 49616\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.35384615384615387\n",
      "Accuracy of '0' predictions:\n",
      "0.21052631578947367\n",
      "Accuracy of '1' predictions:\n",
      "0.8604651162790697\n",
      "number of true buys/sells\n",
      "86 304\n",
      "Attempting model back test\n",
      "Testing day: 3696\n",
      "13/13 [==============================] - 1s 21ms/step\n",
      "Total accuracy:\n",
      "0.7974358974358975\n",
      "Accuracy of '0' predictions:\n",
      "0.7924528301886793\n",
      "Accuracy of '1' predictions:\n",
      "0.8947368421052632\n",
      "number of true buys/sells\n",
      "19 371\n",
      "0.7047974558669536\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "\n",
    "for i in range(40):\n",
    "    #accuracy.append(back_test(back_test_data, 'Records//models//RNN_Final- 24- 0.711.model'))\n",
    "    accuracy.append(back_test(back_test_data, 'Records//Successful Models//AAPL.model'))\n",
    "    \n",
    "print(sum(accuracy) / 40.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "594462f9-7a03-421b-807b-30eefe51ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting model back test\n",
      "Testing index (+ one week): 28552\n",
      "61/61 [==============================] - 2s 17ms/step\n",
      "Total accuracy:\n",
      "0.6194871794871795\n",
      "Accuracy of '0' predictions:\n",
      "0.6577095329494562\n",
      "Accuracy of '1' predictions:\n",
      "0.46511627906976744\n",
      "number of true buys/sells\n",
      "387 1563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46511627906976744"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_test(back_test_data, 'Records//models//RNN_Final- 20- 0.735.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28148a50-326f-47c6-9dd2-651ed985166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trading:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
