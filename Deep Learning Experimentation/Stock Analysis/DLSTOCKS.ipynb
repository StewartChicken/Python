{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d3ccf3-f362-4d32-8ae8-af5e8781ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import bs4 as bs\n",
    "from collections import Counter, deque\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "from sklearn import svm, neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "#Variables\n",
    "tickers = []\n",
    "\n",
    "with open(\"Deep Learning//sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "        \n",
    "with open(\"Deep Learning//AAPLMinute.pickle\", \"rb\") as f:\n",
    "            AAPL_Minute_df = pickle.load(f)\n",
    "        \n",
    "        \n",
    "hm_days = 7\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "NAME = f'{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}'\n",
    "features = ('_Open', '_High', '_Low', '', '_Volume', '_ATR', '_RSI', '_RSI_Weights', '_MA_9', '_MA_50' , '_MA_100', '_MA_200')\n",
    "CORRELATION_COEFFICIENT = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bf9b1a0-0307-400f-8c01-8078a7109f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Classifies if a percent change should be a 1 or a 0\n",
    "#based on the 'requirement' variable\n",
    "def classify(*args):\n",
    "    cols = [c for c in args]\n",
    "    requirement = 0.004\n",
    "    for col in cols:\n",
    "        if col >= requirement:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f69f96-cbc1-4ce7-a6b1-613aa2172548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculates the ATR of a specific stock at any given time\n",
    "def ATR_Column(ticker):\n",
    "    df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df.drop(['Close', 'Open', 'Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    df.rename(columns = {'Adj Close': '{}'.format(ticker), \n",
    "                             'High': '{}_High'.format(ticker),\n",
    "                             'Low': '{}_Low'.format(ticker)}, inplace = True)\n",
    "    atrs = []\n",
    "    true_ranges = []\n",
    "    \n",
    "    df[ticker] = (df[ticker].shift(1))\n",
    "    \n",
    "    true_ranges.append(df['{}_High'.format(ticker)].iloc[0] - df['{}_Low'.format(ticker)].iloc[0])\n",
    "    for i in range(1, len(df)):\n",
    "        ranges = []\n",
    "        ranges.append(df['{}_High'.format(ticker)].iloc[i] - df['{}_Low'.format(ticker)].iloc[i])\n",
    "        ranges.append(abs(df['{}_High'.format(ticker)].iloc[i] - df['{}'.format(ticker)].iloc[i]))\n",
    "        ranges.append(abs(df['{}_Low'.format(ticker)].iloc[i] - df['{}'.format(ticker)].iloc[i]))\n",
    "        true_ranges.append(max(ranges))\n",
    "    \n",
    "    TRS = {'Ranges': true_ranges, 'Date': df.index.values}\n",
    "    true_ranges_DF = pd.DataFrame(data = TRS)\n",
    "    true_ranges_DF.set_index('Date', inplace = True)\n",
    "    \n",
    "    #df = df.join(true_ranges_DF, how = 'outer')\n",
    "    df['ATRS'] = true_ranges_DF['Ranges'].rolling(window = 14, min_periods = 0).sum().div(14)\n",
    "    return df['ATRS']\n",
    "       \n",
    "#ATR_Column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff885fd-5e23-4445-a93f-26c9572ebdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RSI_Column(ticker):\n",
    "    df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df = df['Adj Close'].to_frame()\n",
    "    \n",
    "    close_prices = df.values.tolist()\n",
    "    \n",
    "    changes_list = []\n",
    "    changes_list.append(0)\n",
    "    \n",
    "    for i in range(1, len(close_prices)):\n",
    "        changes_list.append(close_prices[i][0] - close_prices[i - 1][0])\n",
    "       \n",
    "    changes_dict = {'Changes': changes_list, 'Date': df.index.values}\n",
    "    \n",
    "    changes_column = pd.DataFrame(data = changes_dict)\n",
    "    changes_column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['Changes'] = changes_column['Changes']\n",
    "    \n",
    "    #for i in range(1, len(close_prices)):\n",
    "        \n",
    "    gain_list = []\n",
    "    \n",
    "    for i in range(len(close_prices)):\n",
    "        if(df['Changes'].iloc[i] >= 0):\n",
    "            gain_list.append(df['Changes'].iloc[i])\n",
    "        else:\n",
    "            gain_list.append(0.0)\n",
    "    \n",
    "    gain_dict = {'Gain': gain_list, 'Date': df.index.values}\n",
    "    gain_column = pd.DataFrame(data = gain_dict)\n",
    "    gain_column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['Gains'] = gain_column['Gain']\n",
    "            \n",
    "    loss_list = []\n",
    "    \n",
    "    for i in range(len(close_prices)):\n",
    "        if(df['Changes'].iloc[i] < 0):\n",
    "            loss_list.append(abs(df['Changes'].iloc[i]))\n",
    "        else:\n",
    "            loss_list.append(0.0)\n",
    "    \n",
    "    loss_dict = {'Loss': loss_list, 'Date': df.index.values}\n",
    "    loss_column = pd.DataFrame(data = loss_dict)\n",
    "    loss_column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['Loss'] = loss_column['Loss']\n",
    "    \n",
    "    df['Avg_Gain'] = df['Gains'].rolling(window = 14, min_periods = 0).mean()\n",
    "    df['Avg_Loss'] = df['Loss'].rolling(window = 14, min_periods = 0).mean()\n",
    "    \n",
    "    df['RS'] = df['Avg_Gain'] / (df['Avg_Loss'])\n",
    "    df['RS'].iloc[0] = 0\n",
    "    \n",
    "    RSI_List = []\n",
    "    \n",
    "    for i in range(len(close_prices)):\n",
    "        RSI_List.append(100 - (100 / (df['RS'].iloc[i] + 1)))\n",
    "        \n",
    "    RSI_Dict = {'RSI': RSI_List, 'Date': df.index.values}\n",
    "    \n",
    "    RSI_Column = pd.DataFrame(data = RSI_Dict)\n",
    "    RSI_Column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['RSI'] = RSI_Column['RSI']\n",
    "    \n",
    "    weights_list = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if(df['RSI'].iloc[i] >= 70):\n",
    "            weights_list.append(1)\n",
    "        elif(df['RSI'].iloc[i] <= 30):\n",
    "            weights_list.append(-1)\n",
    "        else:\n",
    "            weights_list.append(0)\n",
    "            \n",
    "    weights_dict = {'RSI_Weights': weights_list, 'Date': df.index.values}\n",
    "    \n",
    "    RSI_Weights_Column = pd.DataFrame(data = weights_dict)\n",
    "    RSI_Weights_Column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['RSI_Weights'] = RSI_Weights_Column['RSI_Weights']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df['RSI'], df['RSI_Weights']\n",
    "    \n",
    "#RSI_Column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b223f347-779b-49e0-8fac-e7cb1b1bc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_averages_column(ticker):\n",
    "    df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df = df['Adj Close'].to_frame()\n",
    "    \n",
    "    df['MA_9'] = df['Adj Close'].rolling(window = 9, min_periods = 0).mean()\n",
    "    df['MA_50'] = df['Adj Close'].rolling(window = 50, min_periods = 0).mean()\n",
    "    df['MA_100'] = df['Adj Close'].rolling(window = 100, min_periods = 0).mean()\n",
    "    df['MA_200'] = df['Adj Close'].rolling(window = 200, min_periods = 0).mean()\n",
    "    \n",
    "    return df['MA_9'], df['MA_50'], df['MA_100'], df['MA_200']\n",
    "    \n",
    "#moving_averages_column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c84c82-80e2-411b-8dbe-477d8a2a961d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Saves the list of tickers contained within the SP 500\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    \n",
    "    for row in table.findAll('tr') [1:]:\n",
    "        ticker = row.findAll('td')[0].text.replace('.','-')\n",
    "        ticker = ticker[:-1]\n",
    "        tickers.append(ticker)\n",
    "    with open(\"Deep Learning//sp500tickers.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "\n",
    "#save_sp500_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83fee33-7140-463f-9bfe-6567cddcac21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Retrieves the stock data from each ticker on the sp500 from yahoo\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"Deep Learning//sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('Deep Learning//stock_dfs'):\n",
    "        os.makedirs('Deep Learning//stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2000, 1, 1)\n",
    "    end = dt.datetime(2022, 5, 1)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        path = 'Deep Learning//stock_dfs//{}'.format(ticker)\n",
    "        print(ticker)\n",
    "        if not os.path.exists(path):\n",
    "            df = pdr.get_data_yahoo(ticker, start, end)\n",
    "            df.reset_index(inplace=True)\n",
    "            df.set_index(\"Date\", inplace=True)\n",
    "            df.to_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "#get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4938b06-f552-4e25-81a9-d5ece4c0b4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creates a single dataframe containing all stock data for \n",
    "#Each stock on the sp 500\n",
    "\n",
    "#features = ('_Open', '_High', '_Low', '', '_Volume', '_ATR', '_RSI', '_RSI_Weights', '_MA_9', '_MA_50' , '_MA_100', '_MA_200')\n",
    "\n",
    "def create_joint_df(target_ticker):\n",
    "    main_df = pd.DataFrame()\n",
    "    \n",
    "    for count, ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "        df.set_index('Date', inplace = True)\n",
    "        df.rename(columns = {'Adj Close': '{}'.format(ticker), \n",
    "                             'Open': '{}_Open'.format(ticker),\n",
    "                             'Volume': '{}_Volume'.format(ticker),\n",
    "                             'High': '{}_High'.format(ticker),\n",
    "                             'Low': '{}_Low'.format(ticker)}, inplace = True)\n",
    "        df.drop(['Close'], axis = 1, inplace = True)\n",
    "    \n",
    "    df[f'{target_ticker}_ATR'] = ATR_Column(target_ticker)\n",
    "    df[f'{target_ticker}_RSI'], df[f'{target_ticker}_RSI_Weights'] = RSI_Column(target_ticker)\n",
    "    df[f'{target_ticker}_MA_9'], df[f'{target_ticker}_MA_50'], df[f'{target_ticker}_MA_100'], df[f'{target_ticker}_MA_200'] = moving_averages_column(target_ticker)\n",
    "\n",
    "        \n",
    "        #Extra optimization if needed\n",
    "        '''df[f'{ticker}'] = df[f'{ticker}'].astype(np.float32)\n",
    "        df[f'{ticker}_Open'] = df[f'{ticker}_Open'].astype(np.float32)\n",
    "        df[f'{ticker}_High'] = df[f'{ticker}_High'].astype(np.float32)\n",
    "        df[f'{ticker}_Low'] = df[f'{ticker}_Low'].astype(np.float32)\n",
    "        df[f'{ticker}_Volume'] = df[f'{ticker}_Volume'].astype(np.int32)\n",
    "        df[f'{ticker}_ATR'] = df[f'{ticker}_ATR'].astype(np.float16)\n",
    "        df[f'{ticker}_RSI'] = df[f'{ticker}_RSI'].astype(np.float16)\n",
    "        df[f'{ticker}_RSI_Weights'] = df[f'{ticker}_RSI_Weights'].astype(np.int8)\n",
    "        df[f'{ticker}_MA_9'] = df[f'{ticker}_MA_9'].astype(np.float16)\n",
    "        df[f'{ticker}_MA_50'] = df[f'{ticker}_MA_50'].astype(np.float16)\n",
    "        df[f'{ticker}_MA_100'] = df[f'{ticker}_MA_100'].astype(np.float16)\n",
    "        df[f'{ticker}_MA_200'] = df[f'{ticker}_MA_200'].astype(np.float16)'''\n",
    "        \n",
    "        \n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how = 'outer')\n",
    "        if count % 30 == 0:\n",
    "            print(count)\n",
    "   \n",
    "    main_df.to_csv('Deep Learning//sp500_joined_closes.csv')\n",
    "                         \n",
    "create_joint_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "617b9fb6-2369-40d8-b6ec-c13a054d5578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
      "C:\\Users\\stewa\\AppData\\Local\\Temp\\ipykernel_1892\\127428461.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n"
     ]
    }
   ],
   "source": [
    "def filter_high_correlation(df, ticker, corr_level):\n",
    "    \n",
    "    df_corr = df.corr()\n",
    "    df_corr = df_corr[f'{ticker}']\n",
    "    \n",
    "    for stock_ticker in tickers:\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            if(i == 3):\n",
    "                continue\n",
    "            df_corr = df_corr.drop(f'{stock_ticker}{features[i]}')\n",
    "        \n",
    "    df_corr = df_corr.drop(f'{ticker}')\n",
    "\n",
    "    correlated_tickers = []\n",
    "    \n",
    "    correlated_data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(df_corr)):\n",
    "        if abs(df_corr.iloc[i]) >= corr_level:\n",
    "            correlated_tickers.append(df_corr.index.tolist()[i])\n",
    "    \n",
    "    for stock_ticker in correlated_tickers:\n",
    "        if correlated_data.empty:\n",
    "            correlated_data = df[f'{stock_ticker}{features[0]}'].to_frame()\n",
    "            for feature in range(1, len(features)):\n",
    "                correlated_data = correlated_data.join(df[f'{stock_ticker}{features[feature]}'], how = 'outer')\n",
    "        else:\n",
    "            for feature in range(len(features)):\n",
    "                correlated_data = correlated_data.join(df[f'{stock_ticker}{features[feature]}'], how = 'outer')\n",
    "                \n",
    "    for feature in features:\n",
    "        correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
    "    \n",
    "    correlated_data.to_csv(f'Deep Learning//Correlated_Data//{ticker}-Correlated_stock_data.csv')\n",
    "        \n",
    "    #return correlated_data\n",
    "\n",
    "data = pd.read_csv('Deep Learning//sp500_joined_closes.csv', index_col = 0)\n",
    "data.fillna(0, inplace = True)\n",
    "    \n",
    "data.fillna(0, inplace = True)\n",
    "\n",
    "filter_high_correlation(data, 'AAPL', CORRELATION_COEFFICIENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b93b61f6-28a4-4f5f-8bd1-b642556e540a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5618 entries, 2000-01-03 to 2022-04-29\n",
      "Columns: 194 entries, DHR_Open to AAPL_target\n",
      "dtypes: float64(167), int64(27)\n",
      "memory usage: 8.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DHR_Open</th>\n",
       "      <th>DHR_High</th>\n",
       "      <th>DHR_Low</th>\n",
       "      <th>DHR</th>\n",
       "      <th>DHR_Volume</th>\n",
       "      <th>DHR_ATR</th>\n",
       "      <th>DHR_RSI</th>\n",
       "      <th>DHR_RSI_Weights</th>\n",
       "      <th>DHR_MA_9</th>\n",
       "      <th>DHR_MA_50</th>\n",
       "      <th>...</th>\n",
       "      <th>AAPL_Volume</th>\n",
       "      <th>AAPL_ATR</th>\n",
       "      <th>AAPL_RSI</th>\n",
       "      <th>AAPL_RSI_Weights</th>\n",
       "      <th>AAPL_MA_9</th>\n",
       "      <th>AAPL_MA_50</th>\n",
       "      <th>AAPL_MA_100</th>\n",
       "      <th>AAPL_MA_200</th>\n",
       "      <th>AAPL_future</th>\n",
       "      <th>AAPL_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>9.097801</td>\n",
       "      <td>9.168878</td>\n",
       "      <td>8.718726</td>\n",
       "      <td>5.626462</td>\n",
       "      <td>1474642</td>\n",
       "      <td>0.03217</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>5.625</td>\n",
       "      <td>5.625</td>\n",
       "      <td>...</td>\n",
       "      <td>535796800</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>-0.221105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>8.860879</td>\n",
       "      <td>8.872726</td>\n",
       "      <td>8.434420</td>\n",
       "      <td>5.422277</td>\n",
       "      <td>2424850</td>\n",
       "      <td>0.26400</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "      <td>5.523</td>\n",
       "      <td>5.523</td>\n",
       "      <td>...</td>\n",
       "      <td>512377600</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>0.8230</td>\n",
       "      <td>-0.056098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>8.576573</td>\n",
       "      <td>8.766111</td>\n",
       "      <td>8.410728</td>\n",
       "      <td>5.482780</td>\n",
       "      <td>2854844</td>\n",
       "      <td>0.50300</td>\n",
       "      <td>22.86</td>\n",
       "      <td>-1</td>\n",
       "      <td>5.510</td>\n",
       "      <td>5.510</td>\n",
       "      <td>...</td>\n",
       "      <td>778321600</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>13.710</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8150</td>\n",
       "      <td>0.8150</td>\n",
       "      <td>0.8150</td>\n",
       "      <td>0.8150</td>\n",
       "      <td>-0.034255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>8.493650</td>\n",
       "      <td>8.884572</td>\n",
       "      <td>8.292267</td>\n",
       "      <td>5.543276</td>\n",
       "      <td>1281013</td>\n",
       "      <td>0.74600</td>\n",
       "      <td>37.22</td>\n",
       "      <td>0</td>\n",
       "      <td>5.520</td>\n",
       "      <td>5.520</td>\n",
       "      <td>...</td>\n",
       "      <td>767972800</td>\n",
       "      <td>0.041560</td>\n",
       "      <td>7.523</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7935</td>\n",
       "      <td>0.7935</td>\n",
       "      <td>0.7935</td>\n",
       "      <td>0.7935</td>\n",
       "      <td>0.094079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>8.718726</td>\n",
       "      <td>8.813495</td>\n",
       "      <td>8.671342</td>\n",
       "      <td>5.588651</td>\n",
       "      <td>1751632</td>\n",
       "      <td>0.97950</td>\n",
       "      <td>44.90</td>\n",
       "      <td>0</td>\n",
       "      <td>5.530</td>\n",
       "      <td>5.530</td>\n",
       "      <td>...</td>\n",
       "      <td>460734400</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>24.550</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.7876</td>\n",
       "      <td>0.070980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-25</th>\n",
       "      <td>259.450000</td>\n",
       "      <td>260.880000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>260.460000</td>\n",
       "      <td>4322500</td>\n",
       "      <td>9.26000</td>\n",
       "      <td>30.52</td>\n",
       "      <td>0</td>\n",
       "      <td>274.800</td>\n",
       "      <td>278.200</td>\n",
       "      <td>...</td>\n",
       "      <td>96046400</td>\n",
       "      <td>4.360000</td>\n",
       "      <td>25.920</td>\n",
       "      <td>-1</td>\n",
       "      <td>166.0000</td>\n",
       "      <td>166.8000</td>\n",
       "      <td>169.5000</td>\n",
       "      <td>159.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-26</th>\n",
       "      <td>257.790000</td>\n",
       "      <td>259.470000</td>\n",
       "      <td>250.550000</td>\n",
       "      <td>250.980000</td>\n",
       "      <td>3287500</td>\n",
       "      <td>9.58000</td>\n",
       "      <td>27.67</td>\n",
       "      <td>-1</td>\n",
       "      <td>271.500</td>\n",
       "      <td>277.800</td>\n",
       "      <td>...</td>\n",
       "      <td>95623200</td>\n",
       "      <td>4.510000</td>\n",
       "      <td>23.920</td>\n",
       "      <td>-1</td>\n",
       "      <td>164.8000</td>\n",
       "      <td>166.6000</td>\n",
       "      <td>169.4000</td>\n",
       "      <td>159.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-27</th>\n",
       "      <td>253.340000</td>\n",
       "      <td>257.820000</td>\n",
       "      <td>251.180000</td>\n",
       "      <td>252.280000</td>\n",
       "      <td>2852000</td>\n",
       "      <td>9.46000</td>\n",
       "      <td>27.08</td>\n",
       "      <td>-1</td>\n",
       "      <td>268.000</td>\n",
       "      <td>277.200</td>\n",
       "      <td>...</td>\n",
       "      <td>88063200</td>\n",
       "      <td>4.477000</td>\n",
       "      <td>26.170</td>\n",
       "      <td>-1</td>\n",
       "      <td>163.2000</td>\n",
       "      <td>166.4000</td>\n",
       "      <td>169.2000</td>\n",
       "      <td>159.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-28</th>\n",
       "      <td>255.740000</td>\n",
       "      <td>259.260000</td>\n",
       "      <td>252.120000</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>4352100</td>\n",
       "      <td>9.02000</td>\n",
       "      <td>21.64</td>\n",
       "      <td>-1</td>\n",
       "      <td>265.800</td>\n",
       "      <td>277.200</td>\n",
       "      <td>...</td>\n",
       "      <td>130216800</td>\n",
       "      <td>4.793000</td>\n",
       "      <td>39.030</td>\n",
       "      <td>0</td>\n",
       "      <td>163.1000</td>\n",
       "      <td>166.1000</td>\n",
       "      <td>169.4000</td>\n",
       "      <td>159.2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-29</th>\n",
       "      <td>255.300000</td>\n",
       "      <td>259.600000</td>\n",
       "      <td>250.650000</td>\n",
       "      <td>251.130000</td>\n",
       "      <td>3225100</td>\n",
       "      <td>9.23400</td>\n",
       "      <td>20.17</td>\n",
       "      <td>-1</td>\n",
       "      <td>263.200</td>\n",
       "      <td>276.800</td>\n",
       "      <td>...</td>\n",
       "      <td>131587100</td>\n",
       "      <td>5.223000</td>\n",
       "      <td>35.440</td>\n",
       "      <td>0</td>\n",
       "      <td>162.2000</td>\n",
       "      <td>165.9000</td>\n",
       "      <td>169.2000</td>\n",
       "      <td>159.2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5618 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              DHR_Open    DHR_High     DHR_Low         DHR  DHR_Volume  \\\n",
       "Date                                                                     \n",
       "2000-01-03    9.097801    9.168878    8.718726    5.626462     1474642   \n",
       "2000-01-04    8.860879    8.872726    8.434420    5.422277     2424850   \n",
       "2000-01-05    8.576573    8.766111    8.410728    5.482780     2854844   \n",
       "2000-01-06    8.493650    8.884572    8.292267    5.543276     1281013   \n",
       "2000-01-07    8.718726    8.813495    8.671342    5.588651     1751632   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2022-04-25  259.450000  260.880000  253.000000  260.460000     4322500   \n",
       "2022-04-26  257.790000  259.470000  250.550000  250.980000     3287500   \n",
       "2022-04-27  253.340000  257.820000  251.180000  252.280000     2852000   \n",
       "2022-04-28  255.740000  259.260000  252.120000  257.000000     4352100   \n",
       "2022-04-29  255.300000  259.600000  250.650000  251.130000     3225100   \n",
       "\n",
       "            DHR_ATR  DHR_RSI  DHR_RSI_Weights  DHR_MA_9  DHR_MA_50  ...  \\\n",
       "Date                                                                ...   \n",
       "2000-01-03  0.03217     0.00               -1     5.625      5.625  ...   \n",
       "2000-01-04  0.26400     0.00               -1     5.523      5.523  ...   \n",
       "2000-01-05  0.50300    22.86               -1     5.510      5.510  ...   \n",
       "2000-01-06  0.74600    37.22                0     5.520      5.520  ...   \n",
       "2000-01-07  0.97950    44.90                0     5.530      5.530  ...   \n",
       "...             ...      ...              ...       ...        ...  ...   \n",
       "2022-04-25  9.26000    30.52                0   274.800    278.200  ...   \n",
       "2022-04-26  9.58000    27.67               -1   271.500    277.800  ...   \n",
       "2022-04-27  9.46000    27.08               -1   268.000    277.200  ...   \n",
       "2022-04-28  9.02000    21.64               -1   265.800    277.200  ...   \n",
       "2022-04-29  9.23400    20.17               -1   263.200    276.800  ...   \n",
       "\n",
       "            AAPL_Volume  AAPL_ATR  AAPL_RSI  AAPL_RSI_Weights  AAPL_MA_9  \\\n",
       "Date                                                                       \n",
       "2000-01-03    535796800  0.006897     0.000                -1     0.8594   \n",
       "2000-01-04    512377600  0.016070     0.000                -1     0.8230   \n",
       "2000-01-05    778321600  0.030360    13.710                -1     0.8150   \n",
       "2000-01-06    767972800  0.041560     7.523                -1     0.7935   \n",
       "2000-01-07    460734400  0.053900    24.550                -1     0.7876   \n",
       "...                 ...       ...       ...               ...        ...   \n",
       "2022-04-25     96046400  4.360000    25.920                -1   166.0000   \n",
       "2022-04-26     95623200  4.510000    23.920                -1   164.8000   \n",
       "2022-04-27     88063200  4.477000    26.170                -1   163.2000   \n",
       "2022-04-28    130216800  4.793000    39.030                 0   163.1000   \n",
       "2022-04-29    131587100  5.223000    35.440                 0   162.2000   \n",
       "\n",
       "            AAPL_MA_50  AAPL_MA_100  AAPL_MA_200  AAPL_future  AAPL_target  \n",
       "Date                                                                        \n",
       "2000-01-03      0.8594       0.8594       0.8594    -0.221105            0  \n",
       "2000-01-04      0.8230       0.8230       0.8230    -0.056098            0  \n",
       "2000-01-05      0.8150       0.8150       0.8150    -0.034255            0  \n",
       "2000-01-06      0.7935       0.7935       0.7935     0.094079            1  \n",
       "2000-01-07      0.7876       0.7876       0.7876     0.070980            1  \n",
       "...                ...          ...          ...          ...          ...  \n",
       "2022-04-25    166.8000     169.5000     159.0000          NaN            0  \n",
       "2022-04-26    166.6000     169.4000     159.1000          NaN            0  \n",
       "2022-04-27    166.4000     169.2000     159.1000          NaN            0  \n",
       "2022-04-28    166.1000     169.4000     159.2000          NaN            0  \n",
       "2022-04-29    165.9000     169.2000     159.2000          NaN            0  \n",
       "\n",
       "[5618 rows x 194 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removes NA values and adds a shifted 'future' column\n",
    "#Calls the function to filter out the low correlations\n",
    "def process_data_for_labels(ticker):\n",
    "    #df = pd.read_csv('Deep Learning//sp500_joined_closes.csv', index_col = 0)\n",
    "    #df.fillna(0, inplace = True)\n",
    "    \n",
    "    #df.fillna(0, inplace = True)\n",
    "    \n",
    "    #df = filter_high_correlation(df, ticker, CORRELATION_COEFFICIENT)\n",
    "    \n",
    "    df = pd.read_csv(f'Deep Learning//Correlated_Data//{ticker}-Correlated_stock_data.csv')\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['{}_future'.format(ticker)] = ((df[ticker].shift(-hm_days) - df[ticker]) / df[ticker]) \n",
    "    df['{}_target'.format(ticker)] = list(map(classify,  df['{}_future'.format(ticker)]))\n",
    "\n",
    "    \n",
    "    print(df.info())\n",
    "    return df\n",
    "\n",
    "process_data_for_labels('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028f4ee6-4097-41ee-a09c-2a346d45adcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_df(ticker):\n",
    "    \n",
    "    df = process_data_for_labels('AAPL')\n",
    "    df = df.drop('{}_future'.format(ticker), axis = 1)\n",
    "    \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #Scales/Normalizes data\n",
    "    for col in df.columns:\n",
    "        if col != f'{ticker}_target' or col != f'{ticker}_RSI_Weights':\n",
    "            df[col] = df[col].pct_change()\n",
    "        \n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen = SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "            \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#preprocess_df('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8abdc07-f1f4-45af-8277-b0a1d95af609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_dl(ticker):\n",
    "    X, y = preprocess_df(ticker)\n",
    "    #print(X.dtype, y.dtype)\n",
    "    #print(np.shape(X))\n",
    "  \n",
    "    last_5pct = -int(0.05 * len(X))\n",
    "    \n",
    "    X_test= X[last_5pct:]\n",
    "    X_train = X[:last_5pct]\n",
    "    y_test = y[last_5pct:]\n",
    "    y_train = y[:last_5pct]\n",
    "    \n",
    "    #print(type(X_test), type(y_test))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape = (X_train.shape[1:]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(256, input_shape = (X_train.shape[1:]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(256, input_shape = (X_train.shape[1:])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "              \n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001, decay = 1e-6)\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir = f'Deep Learning/logs/{NAME}')\n",
    "    \n",
    "    filepath = 'RNN_Final-{epoch: 02d}-{val_accuracy: .3f}'\n",
    "    checkpoint = ModelCheckpoint('Deep Learning/models/{}.model'.format(filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max'))\n",
    "    \n",
    "    history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_test, y_test), callbacks = [tensorboard, checkpoint]) \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "20a9762b-ed85-4754-af3e-c232e24c02cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5618 entries, 2000-01-03 to 2022-04-29\n",
      "Columns: 194 entries, DHR_Open to AAPL_target\n",
      "dtypes: float64(167), int64(27)\n",
      "memory usage: 8.4+ MB\n",
      "None\n",
      "Epoch 1/5\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.7115 - accuracy: 0.5931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 1- 0.542.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 1- 0.542.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 18s 221ms/step - loss: 0.7124 - accuracy: 0.5929 - val_loss: 0.6897 - val_accuracy: 0.5418\n",
      "Epoch 2/5\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5643 - accuracy: 0.7171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 2- 0.542.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 2- 0.542.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 8s 202ms/step - loss: 0.5644 - accuracy: 0.7173 - val_loss: 0.6837 - val_accuracy: 0.5418\n",
      "Epoch 3/5\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.5060 - accuracy: 0.7667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 3- 0.594.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 3- 0.594.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 8s 201ms/step - loss: 0.5060 - accuracy: 0.7660 - val_loss: 0.6636 - val_accuracy: 0.5936\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.8148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 4- 0.590.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 4- 0.590.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 7s 192ms/step - loss: 0.4308 - accuracy: 0.8148 - val_loss: 0.6433 - val_accuracy: 0.5896\n",
      "Epoch 5/5\n",
      "36/38 [===========================>..] - ETA: 0s - loss: 0.3914 - accuracy: 0.8168"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 5- 0.618.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Deep Learning/models\\RNN_Final- 5- 0.618.model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 8s 208ms/step - loss: 0.3938 - accuracy: 0.8164 - val_loss: 0.6194 - val_accuracy: 0.6175\n"
     ]
    }
   ],
   "source": [
    "do_dl('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "09e20a7c-717b-4ff9-abf4-2caff90b95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next Goal:\n",
    "\n",
    "#Maybe VWAP\n",
    "#Support resistance algs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
