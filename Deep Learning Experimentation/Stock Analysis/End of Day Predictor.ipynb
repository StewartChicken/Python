{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b47dacb-2af7-49ea-91cb-b9033400bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from collections import Counter, deque\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "from sklearn import svm, neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "#Variables\n",
    "tickers = []\n",
    "with open(\"Deep Learning//sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "        \n",
    "with open(\"Deep Learning//AAPLMinute.pickle\", \"rb\") as f:\n",
    "            AAPL_Minute_df = pickle.load(f)\n",
    "        \n",
    "        \n",
    "hm_days = 7\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "NAME = f'{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}'\n",
    "features = ('_Open', '_High', '_Low', '', '_Volume', '_ATR', '_RSI', '_RSI_Weights', '_MA_9', '_MA_50' , '_MA_100', '_MA_200')\n",
    "CORRELATION_COEFFICIENT = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5af75e96-f3e7-4c27-a4a2-8d71a5d952e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AAPL_Minute_df = AAPL_Minute_df.drop(['t', 'vw', 'n'], axis = 1)\n",
    "#AAPL_Minute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664f77ca-3165-49cc-b35e-4fc05013290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifies if a percent change should be a 1 or a 0\n",
    "#based on the 'requirement' variable\n",
    "def classify(*args):\n",
    "    cols = [c for c in args]\n",
    "    requirement = 0.004\n",
    "    for col in cols:\n",
    "        if col >= requirement:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b7039da-a00a-4647-970c-14897b4e9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the ATR of a specific stock at any given time\n",
    "def ATR_Column(ticker):\n",
    "    df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df.drop(['Close', 'Open', 'Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    df.rename(columns = {'Adj Close': '{}'.format(ticker), \n",
    "                             'High': '{}_High'.format(ticker),\n",
    "                             'Low': '{}_Low'.format(ticker)}, inplace = True)\n",
    "    atrs = []\n",
    "    true_ranges = []\n",
    "    \n",
    "    df[ticker] = (df[ticker].shift(1))\n",
    "    \n",
    "    true_ranges.append(df['{}_High'.format(ticker)].iloc[0] - df['{}_Low'.format(ticker)].iloc[0])\n",
    "    for i in range(1, len(df)):\n",
    "        ranges = []\n",
    "        ranges.append(df['{}_High'.format(ticker)].iloc[i] - df['{}_Low'.format(ticker)].iloc[i])\n",
    "        ranges.append(abs(df['{}_High'.format(ticker)].iloc[i] - df['{}'.format(ticker)].iloc[i]))\n",
    "        ranges.append(abs(df['{}_Low'.format(ticker)].iloc[i] - df['{}'.format(ticker)].iloc[i]))\n",
    "        true_ranges.append(max(ranges))\n",
    "    \n",
    "    TRS = {'Ranges': true_ranges, 'Date': df.index.values}\n",
    "    true_ranges_DF = pd.DataFrame(data = TRS)\n",
    "    true_ranges_DF.set_index('Date', inplace = True)\n",
    "    \n",
    "    #df = df.join(true_ranges_DF, how = 'outer')\n",
    "    df['ATRS'] = true_ranges_DF['Ranges'].rolling(window = 14, min_periods = 0).sum().div(14)\n",
    "    return df['ATRS']\n",
    "       \n",
    "#ATR_Column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96825de6-d90b-4d11-9196-059a19ca0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI_Column(ticker):\n",
    "    df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df = df['Adj Close'].to_frame()\n",
    "    \n",
    "    close_prices = df.values.tolist()\n",
    "    \n",
    "    changes_list = []\n",
    "    changes_list.append(0)\n",
    "    \n",
    "    for i in range(1, len(close_prices)):\n",
    "        changes_list.append(close_prices[i][0] - close_prices[i - 1][0])\n",
    "       \n",
    "    changes_dict = {'Changes': changes_list, 'Date': df.index.values}\n",
    "    \n",
    "    changes_column = pd.DataFrame(data = changes_dict)\n",
    "    changes_column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['Changes'] = changes_column['Changes']\n",
    "    \n",
    "    #for i in range(1, len(close_prices)):\n",
    "        \n",
    "    gain_list = []\n",
    "    \n",
    "    for i in range(len(close_prices)):\n",
    "        if(df['Changes'].iloc[i] >= 0):\n",
    "            gain_list.append(df['Changes'].iloc[i])\n",
    "        else:\n",
    "            gain_list.append(0.0)\n",
    "    \n",
    "    gain_dict = {'Gain': gain_list, 'Date': df.index.values}\n",
    "    gain_column = pd.DataFrame(data = gain_dict)\n",
    "    gain_column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['Gains'] = gain_column['Gain']\n",
    "            \n",
    "    loss_list = []\n",
    "    \n",
    "    for i in range(len(close_prices)):\n",
    "        if(df['Changes'].iloc[i] < 0):\n",
    "            loss_list.append(abs(df['Changes'].iloc[i]))\n",
    "        else:\n",
    "            loss_list.append(0.0)\n",
    "    \n",
    "    loss_dict = {'Loss': loss_list, 'Date': df.index.values}\n",
    "    loss_column = pd.DataFrame(data = loss_dict)\n",
    "    loss_column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['Loss'] = loss_column['Loss']\n",
    "    \n",
    "    df['Avg_Gain'] = df['Gains'].rolling(window = 14, min_periods = 0).mean()\n",
    "    df['Avg_Loss'] = df['Loss'].rolling(window = 14, min_periods = 0).mean()\n",
    "    \n",
    "    df['RS'] = df['Avg_Gain'] / (df['Avg_Loss'])\n",
    "    df['RS'].iloc[0] = 0\n",
    "    \n",
    "    RSI_List = []\n",
    "    \n",
    "    for i in range(len(close_prices)):\n",
    "        RSI_List.append(100 - (100 / (df['RS'].iloc[i] + 1)))\n",
    "        \n",
    "    RSI_Dict = {'RSI': RSI_List, 'Date': df.index.values}\n",
    "    \n",
    "    RSI_Column = pd.DataFrame(data = RSI_Dict)\n",
    "    RSI_Column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['RSI'] = RSI_Column['RSI']\n",
    "    \n",
    "    weights_list = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if(df['RSI'].iloc[i] >= 70):\n",
    "            weights_list.append(1)\n",
    "        elif(df['RSI'].iloc[i] <= 30):\n",
    "            weights_list.append(-1)\n",
    "        else:\n",
    "            weights_list.append(0)\n",
    "            \n",
    "    weights_dict = {'RSI_Weights': weights_list, 'Date': df.index.values}\n",
    "    \n",
    "    RSI_Weights_Column = pd.DataFrame(data = weights_dict)\n",
    "    RSI_Weights_Column.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['RSI_Weights'] = RSI_Weights_Column['RSI_Weights']\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df['RSI'], df['RSI_Weights']\n",
    "    \n",
    "#RSI_Column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a5bc7d-36b2-4ad2-b594-7477d72eba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_averages_column(ticker):\n",
    "    df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df = df['Adj Close'].to_frame()\n",
    "    \n",
    "    df['MA_9'] = df['Adj Close'].rolling(window = 9, min_periods = 0).mean()\n",
    "    df['MA_50'] = df['Adj Close'].rolling(window = 50, min_periods = 0).mean()\n",
    "    df['MA_100'] = df['Adj Close'].rolling(window = 100, min_periods = 0).mean()\n",
    "    df['MA_200'] = df['Adj Close'].rolling(window = 200, min_periods = 0).mean()\n",
    "    \n",
    "    return df['MA_9'], df['MA_50'], df['MA_100'], df['MA_200']\n",
    "    \n",
    "#moving_averages_column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6376db24-2310-417c-adab-a8dd21d90da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the list of tickers contained within the SP 500\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    \n",
    "    for row in table.findAll('tr') [1:]:\n",
    "        ticker = row.findAll('td')[0].text.replace('.','-')\n",
    "        ticker = ticker[:-1]\n",
    "        tickers.append(ticker)\n",
    "    with open(\"Deep Learning//sp500tickers.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tickers, f)\n",
    "\n",
    "#save_sp500_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65f403bf-2c9d-4476-94ae-a78feb849345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMM\n",
      "AOS\n",
      "ABT\n",
      "ABBV\n",
      "ABMD\n",
      "ACN\n",
      "ATVI\n",
      "ADM\n",
      "ADBE\n",
      "ADP\n",
      "AAP\n",
      "AES\n",
      "AFL\n",
      "A\n",
      "APD\n",
      "AKAM\n",
      "ALK\n",
      "ALB\n",
      "ARE\n",
      "ALGN\n",
      "ALLE\n",
      "LNT\n",
      "ALL\n",
      "GOOGL\n",
      "GOOG\n",
      "MO\n",
      "AMZN\n",
      "AMCR\n",
      "AMD\n",
      "AEE\n",
      "AAL\n",
      "AEP\n",
      "AXP\n",
      "AIG\n",
      "AMT\n",
      "AWK\n",
      "AMP\n",
      "ABC\n",
      "AME\n",
      "AMGN\n",
      "APH\n",
      "ADI\n",
      "ANSS\n",
      "ANTM\n",
      "AON\n",
      "APA\n",
      "AAPL\n",
      "AMAT\n",
      "APTV\n",
      "ANET\n",
      "AJG\n",
      "AIZ\n",
      "T\n",
      "ATO\n",
      "ADSK\n",
      "AZO\n",
      "AVB\n",
      "AVY\n",
      "BKR\n",
      "BALL\n",
      "BAC\n",
      "BBWI\n",
      "BAX\n",
      "BDX\n",
      "WRB\n",
      "BRK-B\n",
      "BBY\n",
      "BIO\n",
      "TECH\n",
      "BIIB\n",
      "BLK\n",
      "BK\n",
      "BA\n",
      "BKNG\n",
      "BWA\n",
      "BXP\n",
      "BSX\n",
      "BMY\n",
      "AVGO\n",
      "BR\n",
      "BRO\n",
      "BF-B\n",
      "CHRW\n",
      "CDNS\n",
      "CZR\n",
      "CPT\n",
      "CPB\n",
      "COF\n",
      "CAH\n",
      "KMX\n",
      "CCL\n",
      "CARR\n",
      "CTLT\n",
      "CAT\n",
      "CBOE\n",
      "CBRE\n",
      "CDW\n",
      "CE\n",
      "CNC\n",
      "CNP\n",
      "CDAY\n",
      "CF\n",
      "CRL\n",
      "SCHW\n",
      "CHTR\n",
      "CVX\n",
      "CMG\n",
      "CB\n",
      "CHD\n",
      "CI\n",
      "CINF\n",
      "CTAS\n",
      "CSCO\n",
      "C\n",
      "CFG\n",
      "CTXS\n",
      "CLX\n",
      "CME\n",
      "CMS\n",
      "KO\n",
      "CTSH\n",
      "CL\n",
      "CMCSA\n",
      "CMA\n",
      "CAG\n",
      "COP\n",
      "ED\n",
      "STZ\n",
      "CEG\n",
      "COO\n",
      "CPRT\n",
      "GLW\n",
      "CTVA\n",
      "COST\n",
      "CTRA\n",
      "CCI\n",
      "CSX\n",
      "CMI\n",
      "CVS\n",
      "DHI\n",
      "DHR\n",
      "DRI\n",
      "DVA\n",
      "DE\n",
      "DAL\n",
      "XRAY\n",
      "DVN\n",
      "DXCM\n",
      "FANG\n",
      "DLR\n",
      "DFS\n",
      "DISH\n"
     ]
    },
    {
     "ename": "RemoteDataError",
     "evalue": "Unable to read URL: https://finance.yahoo.com/quote/DISH/history?period1=946728000&period2=1651489199&interval=1d&frequency=1d&filter=history\nResponse Text:\nb'<html><meta charset=\\'utf-8\\'><script>if(window!=window.top){document.write(\\'<p>Content is currently unavailable.</p><img src=\"//geo.yahoo.com/p?s=1197757039&t=\\'+new Date().getTime()+\\'&_R=\\'+encodeURIComponent(document.referrer)+\\'&err=404&err_url=\\'+\\'https%3A%2F%2Ffinance.yahoo.com%2Fquote%2FDISH%2Fhistory%3Fperiod1%3D946728000%26period2%3D1651489199%26interval%3D1d%26frequency%3D1d%26filter%3Dhistory\\'+\\'\" width=\"0px\" height=\"0px\"/>\\');}else{window.location.replace(\\'https://www.yahoo.com/?err=404&err_url=https%3A%2F%2Ffinance.yahoo.com%2Fquote%2FDISH%2Fhistory%3Fperiod1%3D946728000%26period2%3D1651489199%26interval%3D1d%26frequency%3D1d%26filter%3Dhistory\\');}</script><noscript><META http-equiv=\"refresh\" content=\"0;URL=\\'https://www.yahoo.com/?err=404&err_url=https%3A%2F%2Ffinance.yahoo.com%2Fquote%2FDISH%2Fhistory%3Fperiod1%3D946728000%26period2%3D1651489199%26interval%3D1d%26frequency%3D1d%26filter%3Dhistory\\'\"></noscript></html>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDataError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlready have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ticker))\n\u001b[1;32m---> 26\u001b[0m \u001b[43mget_data_from_yahoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mget_data_from_yahoo\u001b[1;34m(reload_sp500)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(ticker)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m---> 18\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpdr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_yahoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m     df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas_datareader\\data.py:80\u001b[0m, in \u001b[0;36mget_data_yahoo\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_yahoo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mYahooDailyReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas_datareader\\base.py:253\u001b[0m, in \u001b[0;36m_DailyBaseReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# If a single symbol, (e.g., 'GOOG')\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbols, (string_types, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m--> 253\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_one_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbols, DataFrame):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas_datareader\\yahoo\\daily.py:149\u001b[0m, in \u001b[0;36mYahooDailyReader._read_one_data\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    147\u001b[0m url \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39mformat(symbol)\n\u001b[1;32m--> 149\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m ptrn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.App\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.main = (.*?);\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn}\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m(this\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas_datareader\\base.py:181\u001b[0m, in \u001b[0;36m_BaseReader._get_response\u001b[1;34m(self, url, params, headers)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_response_text:\n\u001b[0;32m    179\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse Text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(last_response_text)\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RemoteDataError(msg)\n",
      "\u001b[1;31mRemoteDataError\u001b[0m: Unable to read URL: https://finance.yahoo.com/quote/DISH/history?period1=946728000&period2=1651489199&interval=1d&frequency=1d&filter=history\nResponse Text:\nb'<html><meta charset=\\'utf-8\\'><script>if(window!=window.top){document.write(\\'<p>Content is currently unavailable.</p><img src=\"//geo.yahoo.com/p?s=1197757039&t=\\'+new Date().getTime()+\\'&_R=\\'+encodeURIComponent(document.referrer)+\\'&err=404&err_url=\\'+\\'https%3A%2F%2Ffinance.yahoo.com%2Fquote%2FDISH%2Fhistory%3Fperiod1%3D946728000%26period2%3D1651489199%26interval%3D1d%26frequency%3D1d%26filter%3Dhistory\\'+\\'\" width=\"0px\" height=\"0px\"/>\\');}else{window.location.replace(\\'https://www.yahoo.com/?err=404&err_url=https%3A%2F%2Ffinance.yahoo.com%2Fquote%2FDISH%2Fhistory%3Fperiod1%3D946728000%26period2%3D1651489199%26interval%3D1d%26frequency%3D1d%26filter%3Dhistory\\');}</script><noscript><META http-equiv=\"refresh\" content=\"0;URL=\\'https://www.yahoo.com/?err=404&err_url=https%3A%2F%2Ffinance.yahoo.com%2Fquote%2FDISH%2Fhistory%3Fperiod1%3D946728000%26period2%3D1651489199%26interval%3D1d%26frequency%3D1d%26filter%3Dhistory\\'\"></noscript></html>'"
     ]
    }
   ],
   "source": [
    "#Retrieves the stock data from each ticker on the sp500 from yahoo\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"Deep Learning//sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('Deep Learning//stock_dfs'):\n",
    "        os.makedirs('Deep Learning//stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2000, 1, 1)\n",
    "    end = dt.datetime(2022, 5, 1)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        path = 'Deep Learning//stock_dfs//{}'.format(ticker)\n",
    "        print(ticker)\n",
    "        if not os.path.exists(path):\n",
    "            df = pdr.get_data_yahoo(ticker, start, end)\n",
    "            df.reset_index(inplace=True)\n",
    "            df.set_index(\"Date\", inplace=True)\n",
    "            df.to_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43eb17-890b-4c54-a612-7e11aedad3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a single dataframe containing all stock data for \n",
    "#Each stock on the sp 500\n",
    "\n",
    "#features = ('_Open', '_High', '_Low', '', '_Volume', '_ATR', '_RSI', '_RSI_Weights', '_MA_9', '_MA_50' , '_MA_100', '_MA_200')\n",
    "\n",
    "def create_joint_df(target_ticker):\n",
    "    main_df = pd.DataFrame()\n",
    "    \n",
    "    for count, ticker in enumerate(tickers):\n",
    "        df = pd.read_csv('Deep Learning//stock_dfs//{}.csv'.format(ticker.replace('.', '-')))\n",
    "        df.set_index('Date', inplace = True)\n",
    "        df.rename(columns = {'Adj Close': '{}'.format(ticker), \n",
    "                             'Open': '{}_Open'.format(ticker),\n",
    "                             'Volume': '{}_Volume'.format(ticker),\n",
    "                             'High': '{}_High'.format(ticker),\n",
    "                             'Low': '{}_Low'.format(ticker)}, inplace = True)\n",
    "        df.drop(['Close'], axis = 1, inplace = True)\n",
    "   \n",
    "    df[f'{target_ticker}_ATR'] = ATR_Column(target_ticker)\n",
    "    df[f'{target_ticker}_RSI'], df[f'{target_ticker}_RSI_Weights'] = RSI_Column(target_ticker)\n",
    "    df[f'{target_ticker}_MA_9'], df[f'{target_ticker}_MA_50'], df[f'{target_ticker}_MA_100'], df[f'{target_ticker}_MA_200'] = moving_averages_column(target_ticker)\n",
    "\n",
    "\n",
    "    #Extra optimization if needed\n",
    "    '''df[f'{ticker}'] = df[f'{ticker}'].astype(np.float32)\n",
    "    df[f'{ticker}_Open'] = df[f'{ticker}_Open'].astype(np.float32)\n",
    "    df[f'{ticker}_High'] = df[f'{ticker}_High'].astype(np.float32)\n",
    "    df[f'{ticker}_Low'] = df[f'{ticker}_Low'].astype(np.float32)\n",
    "    df[f'{ticker}_Volume'] = df[f'{ticker}_Volume'].astype(np.int32)\n",
    "    df[f'{ticker}_ATR'] = df[f'{ticker}_ATR'].astype(np.float16)\n",
    "    df[f'{ticker}_RSI'] = df[f'{ticker}_RSI'].astype(np.float16)\n",
    "    df[f'{ticker}_RSI_Weights'] = df[f'{ticker}_RSI_Weights'].astype(np.int8)\n",
    "    df[f'{ticker}_MA_9'] = df[f'{ticker}_MA_9'].astype(np.float16)\n",
    "    df[f'{ticker}_MA_50'] = df[f'{ticker}_MA_50'].astype(np.float16)\n",
    "    df[f'{ticker}_MA_100'] = df[f'{ticker}_MA_100'].astype(np.float16)\n",
    "    df[f'{ticker}_MA_200'] = df[f'{ticker}_MA_200'].astype(np.float16)'''\n",
    "\n",
    "        \n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how = 'outer')\n",
    "        if count % 30 == 0:\n",
    "            print(count)\n",
    "   \n",
    "    main_df.to_csv('Deep Learning//sp500_joined_closes.csv')\n",
    "                         \n",
    "create_joint_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe9cd28-7106-4f22-9f30-4a2396d94c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_high_correlation(df, ticker, corr_level):\n",
    "    \n",
    "    df_corr = df.corr()\n",
    "    df_corr = df_corr[f'{ticker}']\n",
    "    \n",
    "    for stock_ticker in tickers:\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            if(i == 3):\n",
    "                continue\n",
    "            df_corr = df_corr.drop(f'{stock_ticker}{features[i]}')\n",
    "        \n",
    "    df_corr = df_corr.drop(f'{ticker}')\n",
    "\n",
    "    correlated_tickers = []\n",
    "    \n",
    "    correlated_data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(df_corr)):\n",
    "        if abs(df_corr.iloc[i]) >= corr_level:\n",
    "            correlated_tickers.append(df_corr.index.tolist()[i])\n",
    "    \n",
    "    for stock_ticker in correlated_tickers:\n",
    "        if correlated_data.empty:\n",
    "            correlated_data = df[f'{stock_ticker}{features[0]}'].to_frame()\n",
    "            for feature in range(1, len(features)):\n",
    "                correlated_data = correlated_data.join(df[f'{stock_ticker}{features[feature]}'], how = 'outer')\n",
    "        else:\n",
    "            for feature in range(len(features)):\n",
    "                correlated_data = correlated_data.join(df[f'{stock_ticker}{features[feature]}'], how = 'outer')\n",
    "                \n",
    "    for feature in features:\n",
    "        correlated_data[f'{ticker}{feature}'] = df[f'{ticker}{feature}']\n",
    "    \n",
    "    correlated_data.to_csv(f'Deep Learning//Correlated_Data//{ticker}-Correlated_stock_data.csv')\n",
    "        \n",
    "    #return correlated_data\n",
    "\n",
    "data = pd.read_csv('Deep Learning//sp500_joined_closes.csv', index_col = 0)\n",
    "data.fillna(0, inplace = True)\n",
    "    \n",
    "data.fillna(0, inplace = True)\n",
    "\n",
    "filter_high_correlation(data, 'AAPL', CORRELATION_COEFFICIENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc1816-0c2e-4b71-8d4a-550fcdece9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes NA values and adds a shifted 'future' column\n",
    "#Calls the function to filter out the low correlations\n",
    "def process_data_for_labels(ticker):\n",
    "    #df = pd.read_csv('Deep Learning//sp500_joined_closes.csv', index_col = 0)\n",
    "    #df.fillna(0, inplace = True)\n",
    "    \n",
    "    #df.fillna(0, inplace = True)\n",
    "    \n",
    "    #df = filter_high_correlation(df, ticker, CORRELATION_COEFFICIENT)\n",
    "    \n",
    "    df = pd.read_csv(f'Deep Learning//Correlated_Data//{ticker}-Correlated_stock_data.csv')\n",
    "    df.set_index('Date', inplace = True)\n",
    "    \n",
    "    df['{}_future'.format(ticker)] = ((df[ticker].shift(-hm_days) - df[ticker]) / df[ticker]) \n",
    "    df['{}_target'.format(ticker)] = list(map(classify,  df['{}_future'.format(ticker)]))\n",
    "\n",
    "    \n",
    "    print(df.info())\n",
    "    return df\n",
    "\n",
    "process_data_for_labels('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7addcf2-e466-4ff9-a166-107116c70719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
