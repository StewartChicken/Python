{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747539c-b10a-4912-acbf-65f782187aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports \n",
    "from collections import Counter, deque\n",
    "import datetime\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "   \n",
    "hm_units = 10 # How many candles into the future the model will predict\n",
    "req = 0.002 # % incresae to be counted as a 'buy' 'hm_units' into the future\n",
    "SEQ_LEN = 60 # How many preceeding candles will be included in each data point \n",
    "BATCH_SIZE = 128 # Initial size of chunks to be fed through Neural Network\n",
    "EPOCHS = 30 # Initial number of epochs the network will run through\n",
    "CORRELATION_COEFFICIENT = 0.92 # (Not used for this model)\n",
    "\n",
    "NAME = f'{SEQ_LEN}-SEQ-{hm_units}-PRED-{int(time.time())}' # Naming convention for model names\n",
    "\n",
    "#data_split_date = '2021-07-22 13:30:00' # Date defining the split between training and testing data\n",
    "\n",
    "#api key that will be used to access data from the source  \n",
    "api = 'gDvQNVWDC2mhOlN1Z9if6JEyDM08CpeC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f5490-a592-4824-a001-e3b24e3cd8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function determines whether the model should buy or sell shares of a company. \n",
    "#One of two values will be returned, either a 0, or a 1\n",
    "#If a '1' is returned, it means that the model predicts that the price will increase by 'req' % \n",
    "#in 'hm_units' candles, and that the price will NOT fall by -'req' % before hitting the \n",
    "#'hm_units' candle. If a '0' is returned, it simply means that neither of the requirements are met, \n",
    "#and the model should not buy shares of the company\n",
    "\n",
    "#The function takes three parameters: the current price, the future price (after 'hm_units' candles, \n",
    "#and the list of future LOW prices that immediately follow the current price. This 'future_prices' list will be exactly 'hm_units' long.\n",
    "def classify(current_price, future_price, future_lows):\n",
    "    \n",
    "    #Sets the threshold value for when a price movement is considered to be significant \n",
    "    requirement = req\n",
    "    \n",
    "    #Creates a list of %changes in relation to 'current_price' for each future low price within 'future_lows' \n",
    "    lows = []\n",
    "    for price in future_lows:\n",
    "        lows.append((price - current_price) / current_price)\n",
    "        \n",
    "    #percent change between 'current_price' and 'future_price'\n",
    "    future_change = (future_price - current_price) / current_price\n",
    "    \n",
    "    #If the last value (value to be predicted) is greater than or equal to the threshold (a significant increase is predicted),\n",
    "    #the method will prepare to return either a 1 (significant increase w/o prior decrease) or a 0 (significant increase *with*\n",
    "    #prior decrease\n",
    "    if future_change >= requirement:\n",
    "        \n",
    "        #Checks to see if any of the values in the 'lows' list falls below the requirement (hit the stop loss of the trade before the shares are sold)\n",
    "        for low in future_lows:\n",
    "            if low <= -requirement:\n",
    "                return 0\n",
    "            \n",
    "        return 1\n",
    "    \n",
    "    \n",
    "    #If, in 'hm_units' candles, there are no significant increases or decreases, the function simply returns a zero\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c4cc3-2222-4f54-9d7f-3c003ae98aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the ATR of a specific stock at any given time (for minute data)\n",
    "def Minute_ATR_Column(ticker):\n",
    "    df = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "    \n",
    "    #Drops the 'open' and 'volume' columns because they are not needed\n",
    "    df.drop([f'{ticker}_Open', f'{ticker}_Volume'], axis = 1, inplace = True)\n",
    "\n",
    "    #List of 'true ranges' for every single candle\n",
    "    true_ranges = []\n",
    "    \n",
    "    #Calculates the true tranges for every candle and adds each one to the 'true_ranges' list\n",
    "    df[f'{ticker}_Close'] = (df[f'{ticker}_Close'].shift(1))\n",
    "    \n",
    "    true_ranges.append(df[f'{ticker}_High'].iloc[0] - df[f'{ticker}_Low'].iloc[0])\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        ranges = []\n",
    "        ranges.append(df[f'{ticker}_High'].iloc[i] - df[f'{ticker}_Low'].iloc[i])\n",
    "        ranges.append(abs(df[f'{ticker}_High'].iloc[i] - df[f'{ticker}_Close'].iloc[i]))\n",
    "        ranges.append(abs(df[f'{ticker}_Low'].iloc[i] - df[f'{ticker}_Close'].iloc[i]))\n",
    "        true_ranges.append(max(ranges))\n",
    "    \n",
    "    #Converts the list of true ranges to a dictionary, then to a dataframe \n",
    "    TRS = {'Ranges': true_ranges, 'Date': df.index.values}\n",
    "    true_ranges_DF = pd.DataFrame(data = TRS)\n",
    "    true_ranges_DF.set_index('Date', inplace = True)\n",
    "    \n",
    "    #Converts each 'true range' into an 'average true range' of the last 14 candles (ATR)\n",
    "    df['ATRS'] = true_ranges_DF['Ranges'].rolling(window = 14, min_periods = 0).sum().div(14)\n",
    "    return df['ATRS']\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1564b-671d-4742-96e1-cd582d8b081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the moving average columns for the minute data. Includes the 9MA, 50MA, 100MA, 200MA\n",
    "#The 'daily' and 'minute' parameters determine which time frame is used to calculate the MA's\n",
    "def moving_averages_column(ticker, daily = False, minute = False):\n",
    "    \n",
    "    if daily:\n",
    "        dfd = pd.read_csv(f'DAILY_STOCK_DATA//{ticker}.csv') #.format(ticker.replace('.', '-')))\n",
    "        dfd = dfd[f'{ticker}_Close'].to_frame()\n",
    "        dfd[f'{ticker}_Daily_MA_9'] = dfd[f'{ticker}_Close'].rolling(window = 9, min_periods = 0).mean()\n",
    "        dfd[f'{ticker}_Daily_MA_50'] = dfd[f'{ticker}_Close'].rolling(window = 50, min_periods = 0).mean()\n",
    "        dfd[f'{ticker}_Daily_MA_100'] = dfd[f'{ticker}_Close'].rolling(window = 100, min_periods = 0).mean()\n",
    "        dfd[f'{ticker}_Daily_MA_200'] = dfd[f'{ticker}_Close'].rolling(window = 200, min_periods = 0).mean()\n",
    "    \n",
    "        return dfd[f'{ticker}_Daily_MA_9'], dfd[f'{ticker}_Daily_MA_50'], dfd[f'{ticker}_Daily_MA_100'], dfd[f'{ticker}_Daily_MA_200']\n",
    "    \n",
    "    if minute:\n",
    "        dfm = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv') #.format(ticker.replace('.', '-')))\n",
    "        dfm = dfm[f'{ticker}_Close'].to_frame()\n",
    "        dfm[f'{ticker}_Minute_MA_9'] = dfm[f'{ticker}_Close'].rolling(window = 9, min_periods = 0).mean()\n",
    "        dfm[f'{ticker}_Minute_MA_50'] = dfm[f'{ticker}_Close'].rolling(window = 50, min_periods = 0).mean()\n",
    "        dfm[f'{ticker}_Minute_MA_100'] = dfm[f'{ticker}_Close'].rolling(window = 100, min_periods = 0).mean()\n",
    "        dfm[f'{ticker}_Minute_MA_200'] = dfm[f'{ticker}_Close'].rolling(window = 200, min_periods = 0).mean()\n",
    "    \n",
    "        return dfm[f'{ticker}_Minute_MA_9'], dfm[f'{ticker}_Minute_MA_50'], dfm[f'{ticker}_Minute_MA_100'], dfm[f'{ticker}_Minute_MA_200']\n",
    "    \n",
    "#moving_averages_column('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfeee7-71dc-429d-bd12-93e6673eb380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will get the daily and minute data for a specific ticker, and it will save it to a CSV to be accessed later.\n",
    "#The desired ticker must be passed as an argument, as well as the start/end date of the data (string). \n",
    "#The 'ignore_after_hours' parameter (boolean) will dictate whether the dataset only contains active hours data, \n",
    "#or if it includes pre/post market. If the pre/post market data is also taken, the user must account for \n",
    "#a high volume of zero volume periods, since the stocks are traded less frequently during these times. \n",
    "#The 'minute' and 'daily' paramaters are booleans which dicate which type of data will be acquired:\n",
    "#daily data or minute data. Both initially set to True\n",
    "\n",
    "#Dates must be in format yyyy-mm-dd\n",
    "def get_ticker_data(ticker, start_date, end_date, ignore_after_hours = True, include_minute = True, include_daily = True):\n",
    "    \n",
    "        print(f'Getting {ticker} data')\n",
    "    \n",
    "        #The 'sort' variable dictates the method by which the data is sorted. asc (Ascending) means the oldest dates are at the top\n",
    "        #and desc (Descending) means the newest dates are at the top. DO NOT CHANGE. This variable was only put in place to make it easier\n",
    "        #for me to develop/test the code while I was writing it. \n",
    "        sort = 'asc' #or desc\n",
    "        \n",
    "        #This variable dictates how many days the 'date_tracker' objecd (created below) will be advanced by each time new data is added\n",
    "        #To the stock minute dataframe\n",
    "        increment = 60\n",
    "        \n",
    "        #This is the file path to store the final Minute stock data\n",
    "        minute_data_path = f'MINUTE_STOCK_DATA//{ticker}'\n",
    "        \n",
    "        #This is the file path to store the final Daily stock data\n",
    "        daily_data_path = f'DAILY_STOCK_DATA//{ticker}'\n",
    "        \n",
    "        #Creates 'start'/'end' strings from the 'start_date' and 'end_date' parameters that can be accessed during execution of the method\n",
    "        start = start_date\n",
    "        end = end_date\n",
    "        \n",
    "        #Creates a datetime object that stores the 'end' date (to be compared to during the while loop)\n",
    "        end_date_time = datetime.datetime(int(end[:4]), int(end[5:7]), int(end[8:10]))\n",
    "        \n",
    "        #This 'date_tracker' object will allow the following while loop determine when to exit the loop\n",
    "        date_tracker = datetime.datetime(int(start[:4]), int(start[5:7]), int(start[8:10]))\n",
    "        \n",
    "        #Url string for the initial 'df1' data\n",
    "        minute_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{str(date_tracker)[:10]}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "        \n",
    "        #If the 'minute' parameter is set to True\n",
    "        if include_minute:\n",
    "    \n",
    "            #Requests the minute stock data from the url listed above and creates the proper dataframe    \n",
    "            firstData = requests.get(minute_url).json()\n",
    "            df1 = pd.DataFrame(firstData['results'])\n",
    "            df1.reset_index(inplace = True)\n",
    "            df1.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "\n",
    "            df1['t'] = pd.to_datetime(df1['t'], unit = 'ms')\n",
    "            df1.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                 'o': f'{ticker}_Open', \n",
    "                                 'h': f'{ticker}_High',\n",
    "                                 'l': f'{ticker}_Low',\n",
    "                                 'v': f'{ticker}_Volume',\n",
    "                                 't': 'Date'}, inplace = True)\n",
    "            \n",
    "            #List of features contained within the initial stock data\n",
    "            features_list = []\n",
    "\n",
    "            #Populates the list of features\n",
    "            for col in df1.columns:\n",
    "                features_list.append(col)\n",
    "\n",
    "            #The list of lists containing data from each feature within the minute stock data. The index of each list will \n",
    "            #correspond with the index of the the list's data's respective feature within the 'features' list\n",
    "            df1_data_list = []\n",
    "\n",
    "            #Appends a list form of each column (feature) of data to the 'df1_data_list'\n",
    "            for col in df1.columns:\n",
    "                df1_data_list.append(df1[col].to_list())\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Collecting and compiling data\")'''\n",
    "\n",
    "            #Since each data call is limited to 50000 candles, this while loop is needed to join multiple 50000 candle chunks of data together\n",
    "            #The 'date_tracker' object will tell the while loop when it should exit the loop, and that is when the 'date-tracker' object's date\n",
    "            #Surpasses the requested end_date_time minus seventy days. The reason the seventy is subtracted, is so that an error is not thrown \n",
    "            #when the date_tracker (which is continually incremented by 70 days throughout the loop) surpasses the requested end_date_time, \n",
    "            #thereby possibly requesting data from the future (which obviously doesn't exist)\n",
    "            while date_tracker < (end_date_time - datetime.timedelta(days = increment)):\n",
    "\n",
    "                #Prints the first date of each obtained chunk of data (since data can only be collected in chunks of 50000)\n",
    "                #print(date_tracker)\n",
    "\n",
    "                #Second url that grabs the data that will be added to the end of the initial 'df1' dataframe\n",
    "                minute_url2 = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{str(date_tracker + datetime.timedelta(days = increment))[:10]}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "\n",
    "                #Acquires the 'to-be-added' data and formats it accordingly\n",
    "                secondData = requests.get(minute_url2).json()\n",
    "                df2 = pd.DataFrame(secondData['results'])\n",
    "                df2.reset_index(inplace = True)\n",
    "                df2.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "                df2['t'] = pd.to_datetime(df2['t'], unit = 'ms')\n",
    "                df2.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                     'o': f'{ticker}_Open', \n",
    "                                     'h': f'{ticker}_High',\n",
    "                                     'l': f'{ticker}_Low',\n",
    "                                     'v': f'{ticker}_Volume',\n",
    "                                     't': 'Date'}, inplace = True)\n",
    "\n",
    "                #Creates a list that will contain list formats of each features' data from the 'df2' dataframe\n",
    "                df2_data_list = []\n",
    "\n",
    "                #Populates the above list\n",
    "                for col in df2.columns:\n",
    "                    df2_data_list.append(df2[col].to_list())\n",
    "\n",
    "                #Appends the contents of the newly aqcuired list to the initial list\n",
    "                for i in range(len(features_list)):\n",
    "                    df1_data_list[i].extend(df2_data_list[i])\n",
    "\n",
    "                #Increments the date_tracker object by 'increment' days\n",
    "                date_tracker += datetime.timedelta(days = increment)\n",
    "\n",
    "            #Progress tracker for the progression of the below code\n",
    "            progress = 0.0\n",
    "\n",
    "            #If the 'ignore_after_hours' paramater is read as True, excecute the following code\n",
    "            if ignore_after_hours:\n",
    "\n",
    "                #List containing all of the indeces that are to be deleted\n",
    "                indeces_to_delete = []\n",
    "\n",
    "                #Status update\n",
    "                '''print(\"Finding all pre/post market data\")'''\n",
    "\n",
    "                #This loop will iterate through every data point in the data and determine the indeces of all \n",
    "                #pre/post market data\n",
    "                for i in range(len(df1_data_list[features_list.index('Date')])):\n",
    "\n",
    "                    #Prints the progress of this loop\n",
    "                    '''if(progress % 50000 == 0):\n",
    "                        print(progress / float(len(df1_data_list[features_list.index('Date')])))\n",
    "\n",
    "                    progress += 1'''\n",
    "\n",
    "                    #Sets the hour/minute values of each iteration\n",
    "                    hour = int(str(df1_data_list[features_list.index('Date')][i])[11:13])\n",
    "                    minute = int(str(df1_data_list[features_list.index('Date')][i])[14:16])\n",
    "\n",
    "                    #Appends the index of all of the candles that occur before 1:30 pm and after\n",
    "                    #8:00 pm (since those are the beginning and end times for the recorded stock data\n",
    "\n",
    "                    if hour < 13:\n",
    "                        indeces_to_delete.append(i)\n",
    "                    elif hour >= 20:\n",
    "                        indeces_to_delete.append(i)\n",
    "                    elif hour == 13 and minute < 30:\n",
    "                        indeces_to_delete.append(i)\n",
    "\n",
    "                #Status update\n",
    "                '''print(\"Deleting all pre/post market data\")'''\n",
    "\n",
    "                #This variable accounts for the shift in the data that occurs every time an index is deleted.\n",
    "                #The 'indeces_to_delete' list contains all of the indeces of candles that occur during pre/post market,\n",
    "                #but every time one of those is deleted in the loop below, the true index of these candles is shifted \n",
    "                #down by one. The 'compensation' variable keeps track of this, and acouunts for it during the filtering process.\n",
    "                compensation = 0\n",
    "\n",
    "                #Tracks the progress of the loop\n",
    "                progress = 0.0\n",
    "\n",
    "                #Iterate through every index of the 'indeces_to_delete' list\n",
    "                for i in indeces_to_delete:\n",
    "\n",
    "                    #Prints the progress to the user\n",
    "                    '''if(progress % 30000 == 0):\n",
    "                        print(progress / float(len(indeces_to_delete)))\n",
    "\n",
    "                    progress += 1'''\n",
    "\n",
    "                    #Deletes the current loop iteration's respective index for every feature within\n",
    "                    #The list of features 'df1_data_list'\n",
    "                    for feature in range(len(features_list)):\n",
    "                        del df1_data_list[feature][i - compensation]\n",
    "\n",
    "                    #Increases 'compensation' by one to account for the shift in data caused by index deletion\n",
    "                    compensation += 1\n",
    "\n",
    "            #Status update\n",
    "            '''print(f'Exporting {ticker} minute data')'''\n",
    "\n",
    "            #Creates the DataFrame containing the minute data and writes it to a CSV File\n",
    "            minute_data_df = pd.DataFrame()\n",
    "\n",
    "            minute_data_df[f'{ticker}_Volume'] = df1_data_list[features_list.index(f'{ticker}_Volume')]\n",
    "            minute_data_df[f'{ticker}_High'] = df1_data_list[features_list.index(f'{ticker}_High')]\n",
    "            minute_data_df[f'{ticker}_Low'] = df1_data_list[features_list.index(f'{ticker}_Low')]\n",
    "            minute_data_df[f'{ticker}_Open'] = df1_data_list[features_list.index(f'{ticker}_Open')]\n",
    "            minute_data_df[f'{ticker}_Close'] = df1_data_list[features_list.index(f'{ticker}_Close')]\n",
    "            minute_data_df['Date'] = df1_data_list[features_list.index('Date')]\n",
    "\n",
    "            minute_data_df.drop_duplicates(inplace = True)\n",
    "\n",
    "            #minute_data_df.to_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "\n",
    "            #Status update\n",
    "            '''print(f'Exporting {ticker} Daily data')'''\n",
    "            \n",
    "        #If the 'daily' parameter is set to True\n",
    "        if include_daily:\n",
    "        \n",
    "            #Beginning of Daily Data collection\n",
    "            daily_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "\n",
    "            daily_data_df = requests.get(daily_url).json()\n",
    "            daily_data_df = pd.DataFrame(daily_data_df['results'])\n",
    "\n",
    "            daily_data_df.reset_index(inplace = True)\n",
    "            daily_data_df.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "\n",
    "            daily_data_df['t'] = pd.to_datetime(daily_data_df['t'], unit = 'ms')\n",
    "            daily_data_df.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                 'o': f'{ticker}_Open', \n",
    "                                 'h': f'{ticker}_High',\n",
    "                                 'l': f'{ticker}_Low',\n",
    "                                 'v': f'{ticker}_Volume',\n",
    "                                 't': 'Date'}, inplace = True)\n",
    "\n",
    "            #daily_data.to_csv(f'DAILY_STOCK_DATA//{ticker}.csv')\n",
    "            \n",
    "        #Returns the proper DateFrames depending on which (or both) of the 'minute'/'daily' parameters are True\n",
    "        if include_minute and include_daily:\n",
    "            return minute_data_df, daily_data_df\n",
    "        elif include_minute:\n",
    "            return minute_data_df\n",
    "        elif include_daily:\n",
    "            return daily_data_df\n",
    "        #If neither the 'minute' nor the 'daily' parameter is True, prints an error statement\n",
    "        else:\n",
    "            print(\"No data type specified\")\n",
    "        '''print(f'Done with {ticker}')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372aff2-4d8b-4113-9d69-5a2f8b73874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ticker_data(ticker, end_date, ignore_after_hours = True, include_minute = True, include_daily = True):\n",
    "    if not include_minute and not include_daily:\n",
    "        print(\"No time window specified\")\n",
    "        \n",
    "    if include_minute:\n",
    "        df1 = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "        df1.drop(['Unnamed: 0'], inplace = True, axis = 1)\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for col in df1.columns:\n",
    "            features_list.append(col)\n",
    "        \n",
    "        start_date = df1['Date'].to_list()[-1][:10]\n",
    "        end = end_date\n",
    "        \n",
    "        sort = 'asc'\n",
    "        \n",
    "        minute_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{start_date}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "  \n",
    "        #Requests the minute stock data from the url listed above and creates the proper dataframe    \n",
    "        minute_data = requests.get(minute_url).json()\n",
    "        df2 = pd.DataFrame(minute_data['results'])\n",
    "        df2.reset_index(inplace = True)\n",
    "        #df2.drop(['vw', 'n', 'index', 'a', 'op'], axis = 1, inplace = True)\n",
    "        df2.drop(['vw', 'n', 'index'], axis = 1, inplace = True)\n",
    "        \n",
    "        df2['t'] = pd.to_datetime(df2['t'], unit = 'ms')\n",
    "        df2.rename(columns = {'c': f'{ticker}_Close', \n",
    "                             'o': f'{ticker}_Open', \n",
    "                             'h': f'{ticker}_High',\n",
    "                             'l': f'{ticker}_Low',\n",
    "                             'v': f'{ticker}_Volume',\n",
    "                             't': 'Date'}, inplace = True)\n",
    "        \n",
    "        features_list2 = []\n",
    "        \n",
    "        for col in df2.columns:\n",
    "            features_list2.append(col)\n",
    "      \n",
    "        df1_data_list = []\n",
    "        \n",
    "        for col in df1.columns:\n",
    "            df1_data_list.append(df1[col].to_list())\n",
    "                \n",
    "        df2_data_list = []\n",
    "        \n",
    "        for col in df2.columns:\n",
    "            df2_data_list.append(df2[col].to_list())\n",
    "            \n",
    "        if ignore_after_hours:\n",
    "\n",
    "            #List containing all of the indeces that are to be deleted\n",
    "            indeces_to_delete = []\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Finding all pre/post market data\")'''\n",
    "            \n",
    "            progress = 0.0\n",
    "\n",
    "            #This loop will iterate through every data point in the data and determine the indeces of all \n",
    "            #pre/post market data\n",
    "            for i in range(len(df2_data_list[features_list.index('Date')])):\n",
    "\n",
    "                #Prints the progress of this loop\n",
    "                '''if(progress % 50000 == 0):\n",
    "                    print(progress / float(len(df1_data_list[features_list.index('Date')])))'''\n",
    "\n",
    "                progress += 1\n",
    "\n",
    "                #Sets the hour/minute values of each iteration\n",
    "                hour = int(str(df2_data_list[features_list.index('Date')][i])[11:13])\n",
    "                minute = int(str(df2_data_list[features_list.index('Date')][i])[14:16])\n",
    "\n",
    "                #Appends the index of all of the candles that occur before 1:30 pm and after\n",
    "                #8:00 pm (since those are the beginning and end times for the recorded stock data\n",
    "\n",
    "                if hour < 13:\n",
    "                    indeces_to_delete.append(i)\n",
    "                elif hour >= 20:\n",
    "                    indeces_to_delete.append(i)\n",
    "                elif hour == 13 and minute < 30:\n",
    "                    indeces_to_delete.append(i)\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Deleting all pre/post market data\")'''\n",
    "\n",
    "            #This variable accounts for the shift in the data that occurs every time an index is deleted.\n",
    "            #The 'indeces_to_delete' list contains all of the indeces of candles that occur during pre/post market,\n",
    "            #but every time one of those is deleted in the loop below, the true index of these candles is shifted \n",
    "            #down by one. The 'compensation' variable keeps track of this, and acouunts for it during the filtering process.\n",
    "            compensation = 0\n",
    "\n",
    "            #Tracks the progress of the loop\n",
    "            progress = 0.0\n",
    "\n",
    "            #Iterate through every index of the 'indeces_to_delete' list\n",
    "            for i in indeces_to_delete:\n",
    "\n",
    "                #Prints the progress to the user\n",
    "                '''if(progress % 30000 == 0):\n",
    "                    print(progress / float(len(indeces_to_delete)))'''\n",
    "\n",
    "                progress += 1\n",
    "\n",
    "                #Deletes the current loop iteration's respective index for every feature within\n",
    "                #The list of features 'df1_data_list'\n",
    "                for feature in range(len(features_list)):\n",
    "                    del df2_data_list[feature][i - compensation]\n",
    "\n",
    "                #Increases 'compensation' by one to account for the shift in data caused by index deletion\n",
    "                compensation += 1\n",
    "\n",
    "        for i in range(len(df2_data_list[features_list2.index('Date')])):\n",
    "            df2_data_list[features_list2.index('Date')][i] = str(df2_data_list[features_list2.index('Date')][i])\n",
    "            \n",
    "        for feature in features_list:\n",
    "            df1_data_list[features_list.index(feature)].extend(df2_data_list[features_list2.index(feature)])\n",
    "            \n",
    "        minute_df = pd.DataFrame()\n",
    "        \n",
    "        minute_df[f'{ticker}_High'] = df1_data_list[features_list.index(f'{ticker}_High')]\n",
    "        minute_df[f'{ticker}_Low'] = df1_data_list[features_list.index(f'{ticker}_Low')]\n",
    "        minute_df[f'{ticker}_Open'] = df1_data_list[features_list.index(f'{ticker}_Open')]\n",
    "        minute_df[f'{ticker}_Close'] = df1_data_list[features_list.index(f'{ticker}_Close')]\n",
    "        minute_df[f'{ticker}_Volume'] = df1_data_list[features_list.index(f'{ticker}_Volume')]\n",
    "        minute_df['Date'] = df1_data_list[features_list.index('Date')]\n",
    "       \n",
    "        minute_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    print(f'Updating {ticker}')\n",
    " \n",
    "    if include_minute and include_daily:\n",
    "        return minute_df, daily_df\n",
    "    elif include_minute:\n",
    "        return minute_df\n",
    "    elif include_daily:\n",
    "        return daily_df\n",
    "    \n",
    "tst = update_ticker_data(ticker = 'AAPL', end_date = '2022-10-10', include_daily = False)\n",
    "tst.to_csv('MINUTE_STOCK_DATA//AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04172d97-c6ce-42c6-8724-9af467140036",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(f'Test_Data//AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfa7f5-3abe-4ab2-bca8-c536e08874ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split_date = '2021-07-22 13:30:00' # Date defining the split between training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362a789-0d2b-4e80-aa6f-1567f81474e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This method processes the minute data for Deep Learning. It adds a 'target_column' which signifies whether the \n",
    "#neural network should predict a long or a short position for a stock.\n",
    "#If split_data is true, the method splits the data into a training and testing set, otherwise it returns all of the data\n",
    "#If read_local is True, it reads data from the computer. If it is false, the 'data' parameter must be\n",
    "#fed the dataframe of data. The 'delete' parameter determines whether the dataframe deletes the 'date' column\n",
    "#from the returned DF\n",
    "def process_minute_data(ticker, start = None, end = None, read_local = True, split_data = True, split_date = None, drop_date = True):\n",
    "    \n",
    "    if read_local:\n",
    "        #Loads the minute data from csv file if read_local is True\n",
    "        df = pd.read_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "\n",
    "        if split_data:\n",
    "            dates_list = df['Date'].to_list()\n",
    "            dates_list = [str(date) for date in dates_list]\n",
    "            split_date_index = dates_list.index(split_date)\n",
    "        \n",
    "        if drop_date:\n",
    "            #Drops the 'Date' and the other extraneous column (idk how it got there I didn't make it), since they will not be needed\n",
    "            df.drop(['Date', 'Unnamed: 0'], axis = 1, inplace = True)\n",
    "    else:\n",
    "        df = get_ticker_data(ticker = ticker, start_date = start, end_date = end, include_daily = False)\n",
    "        \n",
    "        if split_data:\n",
    "            dates_list = df['Date'].to_list()\n",
    "            dates_list = [str(date) for date in dates_list]\n",
    "            split_date_index = dates_list.index(split_date)\n",
    "        \n",
    "        if drop_date:\n",
    "            #Drops the 'Date' and the other extraneous column (idk how it got there I didn't make it), since they will not be needed\n",
    "            df.drop(['Date'], axis = 1, inplace = True)\n",
    "\n",
    "    #This list will contain lists of the future values for a specific candle. Each list within 'future_column' will\n",
    "    #be 'hm_units' long. This list is never put into the data frame that will be fed through the neural network. It is\n",
    "    #created for the soul purpose of creating the 'target_column' which tells the Neural network whether or not its guess\n",
    "    #was correct. Each list of futures within this list will befed through the 'classify' method, along with the close \n",
    "    #price of the candle that directly preceeds the first index of the list. Again, this is only a temporary list, and \n",
    "    #will be discarded after the function has completed its process. The 'future_column' list will also be 'hm_units'\n",
    "    #shorter than the final data frame bc the 'classify' function can't classify values that run outside of the index\n",
    "    #of the close prices. (ie, if the 'classify' function must create a result 'hm_units' into the future, and it \n",
    "    #is fed the very last data point of the close prices, there is no future data to calculate %discrepency against.\n",
    "    future_lows_column = []\n",
    "    \n",
    "    #This loop appends a list of 'hm_units' future values to the 'future_column' list\n",
    "    for i in range(len(df) - hm_units):\n",
    "        \n",
    "        #List of future lows to be appended to the 'future_lows_column'\n",
    "        future_lows = []\n",
    "        \n",
    "        #Appends each future value to the 'futures' list, skipping the first (since that is the current value)\n",
    "        for ii in range(1, hm_units + 1):\n",
    "            future_lows.append(df[f'{ticker}_Low'].iloc[i + ii])\n",
    "    \n",
    "        future_lows_column.append(future_lows)\n",
    "    \n",
    "    #Target column which will be added to the data frame. This column essentially uses the 'classify' function\n",
    "    #to calculate whether a stock will significantly increase or decrease during the next 'hm_units' minute candles.\n",
    "    target_column = []\n",
    "    \n",
    "    #Feeds every close value of the current ticker and every list within the 'future_lows_column' list through the 'classify'\n",
    "    #function, and appends it to the 'target_column'\n",
    "    for i in range(len(future_lows_column)):\n",
    "        target_column.append(classify(df[f'{ticker}_Close'].iloc[i], df[f'{ticker}_Close'].iloc[i + hm_units], future_lows_column[i]))\n",
    "    \n",
    "    #Since the 'target_column' can't make predictions with data it doesn't have (when reaching the end of the close prices,\n",
    "    #The last 'hm_units' of the 'target_column' is simply filled with a zero. Bc each dataframe has roughly 300k to 400k\n",
    "    #datapoints, these zeros should not affect accuracy in the slightest\n",
    "    for i in range(hm_units):\n",
    "        target_column.append(0)\n",
    "        \n",
    "    \n",
    "    #Adds the minute atr column to the data frame\n",
    "    df[f'{ticker}_Minute_ATR'] = Minute_ATR_Column(ticker)\n",
    "    \n",
    "    #Adds a 9, 50, 100, and 200 MA to the model\n",
    "    df[f'{ticker}_9_MA'], df[f'{ticker}_50_MA'], df[f'{ticker}_100_MA'], df[f'{ticker}_200_MA'] = moving_averages_column(ticker, minute = True)\n",
    "    \n",
    "    #Adds the 'target_column' list to the dataframe, and returns the dataframe\n",
    "    df[f'{ticker}_target'] = target_column\n",
    "    \n",
    "    if split_data:\n",
    "        #Splits the data into training and testing data\n",
    "        train_df = df.iloc[:split_date_index]\n",
    "        test_df = df.iloc[split_date_index:]\n",
    "\n",
    "        #Writes the testing data to a csv to be accessed later\n",
    "        test_df.to_csv(f'Test_Data//{ticker}.csv')\n",
    "  \n",
    "        return train_df\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "#tst = process_minute_data('AAPL', start = '2022-06-01', end = '2022-08-01', read_local = False, split_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba6249-ad72-4832-a262-a65821f0db2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reformat_minute_for_DL(ticker, start_date = None, end_date = None, read_local = True, split_data = True, split_date = None, drop_date = True):\n",
    "    \n",
    "    #Data processing\n",
    "    \n",
    "    df = process_minute_data(ticker = ticker, start = start_date, end = end_date, read_local = read_local, split_data = split_data, split_date = split_date, drop_date = drop_date)\n",
    "   \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #Scales/Normalizes data\n",
    "    for col in df.columns:\n",
    "        #if col != f'{ticker}_target' or col != f'{ticker}_RSI_Weights':\n",
    "        if col != f'{ticker}_target':\n",
    "            df[col] = df[col].pct_change()\n",
    "        \n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    \n",
    "    #Dictionary containing the data (current minute information as well as the previous 60 minutes of data) and the target\n",
    "    sequential_data = []\n",
    "    prev_units = deque(maxlen = SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_units.append([n for n in i[:-1]])\n",
    "        if len(prev_units) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_units), i[-1]])\n",
    "    \n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    #Balances the data so that there are an equal number of 1's and 0's (otherwise the model will attempt to \n",
    "    #achieve a higher accuracy by simply guessing the more frequently appearing value\n",
    "    buys = []\n",
    "    sells = []\n",
    " \n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])    \n",
    "   \n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    #Shows the user that the model has been balanced\n",
    "    print(\"Num buys/sells:\")\n",
    "    print(len(buys))\n",
    "    print(len(sells))\n",
    "\n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    #X contains the data and y contains the targets\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#tst = reformat_minute_for_DL(ticker = 'AAPL', start_date = '2022-08-01', end_date = '2022-08-07', read_local = False, split_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3823de7-9078-4c64-ba4d-74af2503ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = reformat_minute_for_DL(ticker = 'AAPL', start_date = '2022-04-01', end_date = '2022-10-10', read_local = True, split_data = True, split_date = '2022-07-22 13:30:00', drop_date = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff2141-da16-4bc8-825f-267d0f5adfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_dl(ticker):\n",
    "    X, y = reformat_minute_for_DL(ticker)\n",
    "    \n",
    "    #Splits the data into training data and validation data\n",
    "    last_5pct = -int(len(X) * 0.05)\n",
    "    \n",
    "    X_train = X[:last_5pct]\n",
    "    y_train = y[:last_5pct]\n",
    "    X_test = X[last_5pct:]\n",
    "    y_test = y[last_5pct:]\n",
    "\n",
    "    print(\"Number of training data points:\")\n",
    "    print(len(X_train))\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape = (X_train.shape[1:]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(64, input_shape = (X_train.shape[1:]), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(LSTM(64, input_shape = (X_train.shape[1:])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "              \n",
    "    model.add(Dense(256, activation = 'relu')) #512\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001, decay = 1e-6)\n",
    "    \n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt, metrics = ['accuracy']) \n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir = f'Records/logs/{NAME}')\n",
    "    \n",
    "    filepath = 'RNN_Final-{epoch: 02d}-{val_accuracy: .3f}'\n",
    "    checkpoint = ModelCheckpoint('Records/models/{}.model'.format(filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max'))\n",
    "  \n",
    "    history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_test, y_test), callbacks = [tensorboard, checkpoint]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ae5f01c-41a0-4d60-bd0e-935a020ef2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Back testing\n",
    "\n",
    "def back_test(ticker, model_location):\n",
    "    \n",
    "    print(\"Attempting model back test\")\n",
    "    \n",
    "    #Variable used to deterime how many candles of data the model will backtest (390 = one day)\n",
    "    num_candles = 390\n",
    "    \n",
    "    #Processing the testing data\n",
    "    df = pd.read_csv(f'Test_Data//{ticker}.csv')\n",
    "    df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "    \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #Scales/Normalizes data\n",
    "    for col in df.columns:\n",
    "        #if col != f'{ticker}_target' or col != f'{ticker}_RSI_Weights':\n",
    "        if col != f'{ticker}_target':\n",
    "            df[col] = df[col].pct_change()\n",
    "        \n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    sequential_data = []\n",
    "    prev_units = deque(maxlen = SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_units.append([n for n in i[:-1]])\n",
    "        if len(prev_units) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_units), i[-1]])\n",
    "            \n",
    "    index = random.randrange(len(sequential_data) - num_candles)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(num_candles):\n",
    "        X.append(sequential_data[index + i][0])\n",
    "        y.append(sequential_data[index + i][1])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "\n",
    "    # Testing candle number 'index' and the next 389 candles as well (one day)\n",
    "    print(f\"Testing candle + 389: {index}\")\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_location)\n",
    "\n",
    "    #Creates list of prediction probabilities (the model will create a list that contains the probablity\n",
    "    #of the data outputting any specific classification. The max value will be returned as the model's guess\n",
    "    prediction_probabilities = model.predict(X)\n",
    "    predictions = np.argmax(prediction_probabilities, axis = 1)\n",
    "\n",
    "    #Accuracy validation \n",
    "    \n",
    "    total_count = 0.0\n",
    "    count0 = 0.0\n",
    "    count1 = 0.0\n",
    "\n",
    "    print(\"Total accuracy:\")\n",
    "    for i in range(len(y)):\n",
    "        if(predictions[i] == y[i]):\n",
    "            total_count += 1.0\n",
    "\n",
    "    print(total_count / float(len(y)))\n",
    "\n",
    "    indeces_0 = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0:\n",
    "            indeces_0.append(i)\n",
    "\n",
    "    print(\"Accuracy of '0' predictions:\")\n",
    "    for i in indeces_0:\n",
    "        if(predictions[i] == 0):\n",
    "            count0 += 1.0\n",
    "\n",
    "    print(count0 / float(len(indeces_0)))\n",
    "\n",
    "    indeces_1 = []\n",
    "\n",
    "    print(\"Accuracy of '1' predictions:\")\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            indeces_1.append(i)\n",
    "\n",
    "    for i in indeces_1:\n",
    "        if(predictions[i] == 1):\n",
    "            count1 += 1.0\n",
    "\n",
    "    print(count1 / float(len(indeces_1)))\n",
    "    \n",
    "    print(\"number of true buys/sells\")\n",
    "    print(len(indeces_1), len(indeces_0))\n",
    "    \n",
    "    return count1 / float(len(indeces_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03235f-5d5f-43e7-9cdf-014cfd80a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 #128 - original --- 32 = optimized (potentially)\n",
    "LSIZE = 128 #128 - original --- 64 = optimized (porentially)\n",
    "EPOCHS = 20\n",
    "hm_units = 10\n",
    "req = .002\n",
    "\n",
    "#do_dl('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a49210-97c0-4134-98da-77f82e9bb4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for i in range(40):\n",
    "    #accuracy.append(back_test(back_test_data, 'Records//models//RNN_Final- 24- 0.711.model'))\n",
    "    accuracy.append(back_test('AAPL', 'Records//Successful Models//AAPL.model'))\n",
    "    \n",
    "print(sum(accuracy) / 40.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0adb59a4-32ca-454d-a7d3-f5a60449e82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting model back test\n",
      "Testing candle + 389: 8911\n",
      "13/13 [==============================] - 2s 28ms/step\n",
      "Total accuracy:\n",
      "0.09230769230769231\n",
      "Accuracy of '0' predictions:\n",
      "0.0\n",
      "Accuracy of '1' predictions:\n",
      "1.0\n",
      "number of true buys/sells\n",
      "36 354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#back_test('AAPL', 'Records//models//RNN_Final- 20- 0.714.model')\n",
    "back_test('AAPL', 'Records//Successful Models//AAPL.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b615d9-7fb6-4339-9c9c-a10e629131c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trading(ticker, start, end):\n",
    "    df = get_ticker_data(ticker = ticker, start_date = start, end_date = end, include_daily = False)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
