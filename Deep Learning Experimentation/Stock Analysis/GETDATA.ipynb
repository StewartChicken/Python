{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0487185b-b8be-4dd8-9847-bf758f208830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "with open(\"sp500tickers_unrevised.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "       \n",
    "\n",
    "    \n",
    "dropped_tickers = ('BALL', 'BRK-B', 'BF-B', 'KMX', 'WBD', 'CEG', 'PARA', 'WTW', 'META', 'OGN', 'BBWI')\n",
    "\n",
    "for i in dropped_tickers:\n",
    "    if i in tickers:\n",
    "        del tickers[tickers.index(i)]\n",
    "        \n",
    "hm_units = 10\n",
    "req = 0.002\n",
    "SEQ_LEN = 60\n",
    "BATCH_SIZE = 132\n",
    "EPOCHS = 30\n",
    "NAME = f'{SEQ_LEN}-SEQ-{hm_units}-PRED-{int(time.time())}'\n",
    "CORRELATION_COEFFICIENT = 0.92\n",
    "\n",
    "#api keys that will be used to access data from the source\n",
    "        \n",
    "api = 'gDvQNVWDC2mhOlN1Z9if6JEyDM08CpeC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600dda9f-c35f-4002-98c6-cd56a34da646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will get the daily and minute data for a specific ticker, and it will save it to a CSV to be accessed later.\n",
    "#The desired ticker must be passed as an argument, as well as the start/end date of the data (string). \n",
    "#The 'ignore_after_hours' parameter (boolean) will dictate whether the dataset only contains active hours data, \n",
    "#or if it includes pre/post market. If the pre/post market data is also taken, the user must account for \n",
    "#a high volume of zero volume periods, since the stocks are traded less frequently during these times. \n",
    "#The 'minute' and 'daily' paramaters are booleans which dicate which type of data will be acquired:\n",
    "#daily data or minute data. Both initially set to True\n",
    "\n",
    "#Dates must be in format yyyy-mm-dd\n",
    "def get_ticker_data(ticker, start_date, end_date, ignore_after_hours = True, include_minute = True, include_daily = True):\n",
    "    \n",
    "        print(f'Getting {ticker} data')\n",
    "    \n",
    "        #The 'sort' variable dictates the method by which the data is sorted. asc (Ascending) means the oldest dates are at the top\n",
    "        #and desc (Descending) means the newest dates are at the top. DO NOT CHANGE. This variable was only put in place to make it easier\n",
    "        #for me to develop/test the code while I was writing it. \n",
    "        sort = 'asc' #or desc\n",
    "        \n",
    "        #This variable dictates how many days the 'date_tracker' objecd (created below) will be advanced by each time new data is added\n",
    "        #To the stock minute dataframe\n",
    "        increment = 60\n",
    "        \n",
    "        #This is the file path to store the final Minute stock data\n",
    "        minute_data_path = f'MINUTE_STOCK_DATA//{ticker}'\n",
    "        \n",
    "        #This is the file path to store the final Daily stock data\n",
    "        daily_data_path = f'DAILY_STOCK_DATA//{ticker}'\n",
    "        \n",
    "        #Creates 'start'/'end' strings from the 'start_date' and 'end_date' parameters that can be accessed during execution of the method\n",
    "        start = start_date\n",
    "        end = end_date\n",
    "        \n",
    "        #Creates a datetime object that stores the 'end' date (to be compared to during the while loop)\n",
    "        end_date_time = datetime.datetime(int(end[:4]), int(end[5:7]), int(end[8:10]))\n",
    "        \n",
    "        #This 'date_tracker' object will allow the following while loop determine when to exit the loop\n",
    "        date_tracker = datetime.datetime(int(start[:4]), int(start[5:7]), int(start[8:10]))\n",
    "        \n",
    "        #Url string for the initial 'df1' data\n",
    "        minute_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{str(date_tracker)[:10]}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "        \n",
    "        #If the 'minute' parameter is set to True\n",
    "        if include_minute:\n",
    "    \n",
    "            #Requests the minute stock data from the url listed above and creates the proper dataframe    \n",
    "            firstData = requests.get(minute_url).json()\n",
    "            df1 = pd.DataFrame(firstData['results'])\n",
    "            df1.reset_index(inplace = True)\n",
    "            df1.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "\n",
    "            df1['t'] = pd.to_datetime(df1['t'], unit = 'ms')\n",
    "            df1.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                 'o': f'{ticker}_Open', \n",
    "                                 'h': f'{ticker}_High',\n",
    "                                 'l': f'{ticker}_Low',\n",
    "                                 'v': f'{ticker}_Volume',\n",
    "                                 't': 'Date'}, inplace = True)\n",
    "            \n",
    "            #List of features contained within the initial stock data\n",
    "            features_list = []\n",
    "\n",
    "            #Populates the list of features\n",
    "            for col in df1.columns:\n",
    "                features_list.append(col)\n",
    "\n",
    "            #The list of lists containing data from each feature within the minute stock data. The index of each list will \n",
    "            #correspond with the index of the the list's data's respective feature within the 'features' list\n",
    "            df1_data_list = []\n",
    "\n",
    "            #Appends a list form of each column (feature) of data to the 'df1_data_list'\n",
    "            for col in df1.columns:\n",
    "                df1_data_list.append(df1[col].to_list())\n",
    "\n",
    "            #Status update\n",
    "            '''print(\"Collecting and compiling data\")'''\n",
    "\n",
    "            #Since each data call is limited to 50000 candles, this while loop is needed to join multiple 50000 candle chunks of data together\n",
    "            #The 'date_tracker' object will tell the while loop when it should exit the loop, and that is when the 'date-tracker' object's date\n",
    "            #Surpasses the requested end_date_time minus seventy days. The reason the seventy is subtracted, is so that an error is not thrown \n",
    "            #when the date_tracker (which is continually incremented by 70 days throughout the loop) surpasses the requested end_date_time, \n",
    "            #thereby possibly requesting data from the future (which obviously doesn't exist)\n",
    "            while date_tracker < (end_date_time - datetime.timedelta(days = increment)):\n",
    "\n",
    "                #Prints the first date of each obtained chunk of data (since data can only be collected in chunks of 50000)\n",
    "                #print(date_tracker)\n",
    "\n",
    "                #Second url that grabs the data that will be added to the end of the initial 'df1' dataframe\n",
    "                minute_url2 = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/minute/{str(date_tracker + datetime.timedelta(days = increment))[:10]}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "\n",
    "                #Acquires the 'to-be-added' data and formats it accordingly\n",
    "                secondData = requests.get(minute_url2).json()\n",
    "                df2 = pd.DataFrame(secondData['results'])\n",
    "                df2.reset_index(inplace = True)\n",
    "                df2.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "                df2['t'] = pd.to_datetime(df2['t'], unit = 'ms')\n",
    "                df2.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                     'o': f'{ticker}_Open', \n",
    "                                     'h': f'{ticker}_High',\n",
    "                                     'l': f'{ticker}_Low',\n",
    "                                     'v': f'{ticker}_Volume',\n",
    "                                     't': 'Date'}, inplace = True)\n",
    "\n",
    "                #Creates a list that will contain list formats of each features' data from the 'df2' dataframe\n",
    "                df2_data_list = []\n",
    "\n",
    "                #Populates the above list\n",
    "                for col in df2.columns:\n",
    "                    df2_data_list.append(df2[col].to_list())\n",
    "\n",
    "                #Appends the contents of the newly aqcuired list to the initial list\n",
    "                for i in range(len(features_list)):\n",
    "                    df1_data_list[i].extend(df2_data_list[i])\n",
    "\n",
    "                #Increments the date_tracker object by 'increment' days\n",
    "                date_tracker += datetime.timedelta(days = increment)\n",
    "\n",
    "            #Progress tracker for the progression of the below code\n",
    "            progress = 0.0\n",
    "\n",
    "            #If the 'ignore_after_hours' paramater is read as True, excecute the following code\n",
    "            if ignore_after_hours:\n",
    "\n",
    "                #List containing all of the indeces that are to be deleted\n",
    "                indeces_to_delete = []\n",
    "\n",
    "                #Status update\n",
    "                '''print(\"Finding all pre/post market data\")'''\n",
    "\n",
    "                #This loop will iterate through every data point in the data and determine the indeces of all \n",
    "                #pre/post market data\n",
    "                for i in range(len(df1_data_list[features_list.index('Date')])):\n",
    "\n",
    "                    #Prints the progress of this loop\n",
    "                    '''if(progress % 50000 == 0):\n",
    "                        print(progress / float(len(df1_data_list[features_list.index('Date')])))\n",
    "\n",
    "                    progress += 1'''\n",
    "\n",
    "                    #Sets the hour/minute values of each iteration\n",
    "                    hour = int(str(df1_data_list[features_list.index('Date')][i])[11:13])\n",
    "                    minute = int(str(df1_data_list[features_list.index('Date')][i])[14:16])\n",
    "\n",
    "                    #Appends the index of all of the candles that occur before 1:30 pm and after\n",
    "                    #8:00 pm (since those are the beginning and end times for the recorded stock data\n",
    "\n",
    "                    if hour < 13:\n",
    "                        indeces_to_delete.append(i)\n",
    "                    elif hour >= 20:\n",
    "                        indeces_to_delete.append(i)\n",
    "                    elif hour == 13 and minute < 30:\n",
    "                        indeces_to_delete.append(i)\n",
    "\n",
    "                #Status update\n",
    "                '''print(\"Deleting all pre/post market data\")'''\n",
    "\n",
    "                #This variable accounts for the shift in the data that occurs every time an index is deleted.\n",
    "                #The 'indeces_to_delete' list contains all of the indeces of candles that occur during pre/post market,\n",
    "                #but every time one of those is deleted in the loop below, the true index of these candles is shifted \n",
    "                #down by one. The 'compensation' variable keeps track of this, and acouunts for it during the filtering process.\n",
    "                compensation = 0\n",
    "\n",
    "                #Tracks the progress of the loop\n",
    "                progress = 0.0\n",
    "\n",
    "                #Iterate through every index of the 'indeces_to_delete' list\n",
    "                for i in indeces_to_delete:\n",
    "\n",
    "                    #Prints the progress to the user\n",
    "                    '''if(progress % 30000 == 0):\n",
    "                        print(progress / float(len(indeces_to_delete)))\n",
    "\n",
    "                    progress += 1'''\n",
    "\n",
    "                    #Deletes the current loop iteration's respective index for every feature within\n",
    "                    #The list of features 'df1_data_list'\n",
    "                    for feature in range(len(features_list)):\n",
    "                        del df1_data_list[feature][i - compensation]\n",
    "\n",
    "                    #Increases 'compensation' by one to account for the shift in data caused by index deletion\n",
    "                    compensation += 1\n",
    "\n",
    "            #Status update\n",
    "            '''print(f'Exporting {ticker} minute data')'''\n",
    "\n",
    "            #Creates the DataFrame containing the minute data and writes it to a CSV File\n",
    "            minute_data_df = pd.DataFrame()\n",
    "\n",
    "            minute_data_df[f'{ticker}_Volume'] = df1_data_list[features_list.index(f'{ticker}_Volume')]\n",
    "            minute_data_df[f'{ticker}_High'] = df1_data_list[features_list.index(f'{ticker}_High')]\n",
    "            minute_data_df[f'{ticker}_Low'] = df1_data_list[features_list.index(f'{ticker}_Low')]\n",
    "            minute_data_df[f'{ticker}_Open'] = df1_data_list[features_list.index(f'{ticker}_Open')]\n",
    "            minute_data_df[f'{ticker}_Close'] = df1_data_list[features_list.index(f'{ticker}_Close')]\n",
    "            minute_data_df['Date'] = df1_data_list[features_list.index('Date')]\n",
    "\n",
    "            minute_data_df.drop_duplicates(inplace = True)\n",
    "\n",
    "            #minute_data_df.to_csv(f'MINUTE_STOCK_DATA//{ticker}.csv')\n",
    "\n",
    "            #Status update\n",
    "            '''print(f'Exporting {ticker} Daily data')'''\n",
    "            \n",
    "        #If the 'daily' parameter is set to True\n",
    "        if include_daily:\n",
    "        \n",
    "            #Beginning of Daily Data collection\n",
    "            daily_url = f'https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start}/{end}?adjusted=true&sort={sort}&limit=50000&apiKey={api}'\n",
    "\n",
    "            daily_data_df = requests.get(daily_url).json()\n",
    "            daily_data_df = pd.DataFrame(daily_data_df['results'])\n",
    "\n",
    "            daily_data_df.reset_index(inplace = True)\n",
    "            daily_data_df.drop(['vw', 'n'], axis = 1, inplace = True)\n",
    "\n",
    "            daily_data_df['t'] = pd.to_datetime(daily_data_df['t'], unit = 'ms')\n",
    "            daily_data_df.rename(columns = {'c': f'{ticker}_Close', \n",
    "                                 'o': f'{ticker}_Open', \n",
    "                                 'h': f'{ticker}_High',\n",
    "                                 'l': f'{ticker}_Low',\n",
    "                                 'v': f'{ticker}_Volume',\n",
    "                                 't': 'Date'}, inplace = True)\n",
    "\n",
    "            #daily_data.to_csv(f'DAILY_STOCK_DATA//{ticker}.csv')\n",
    "            \n",
    "        #Returns the proper DateFrames depending on which (or both) of the 'minute'/'daily' parameters are True\n",
    "        if include_minute and include_daily:\n",
    "            return minute_data_df, daily_data_df\n",
    "        elif include_minute:\n",
    "            return minute_data_df\n",
    "        elif include_daily:\n",
    "            return daily_data_df\n",
    "        #If neither the 'minute' nor the 'daily' parameter is True, prints an error statement\n",
    "        else:\n",
    "            print(\"No data type specified\")\n",
    "        '''print(f'Done with {ticker}')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45a96cc-cc0b-4a8c-9aa2-c0bef6488d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting FRC data\n",
      "Getting FE data\n",
      "Getting FIS data\n",
      "Getting FISV data\n",
      "Getting FLT data\n",
      "Getting FMC data\n",
      "Getting F data\n",
      "Getting FTNT data\n",
      "Getting FTV data\n",
      "Getting FBHS data\n",
      "Getting FOXA data\n",
      "Getting FOX data\n",
      "Getting BEN data\n",
      "Getting FCX data\n",
      "Getting GRMN data\n",
      "Getting IT data\n",
      "Getting GE data\n",
      "Getting GNRC data\n",
      "Getting GD data\n",
      "Getting GIS data\n",
      "Getting GM data\n",
      "Getting GPC data\n",
      "Getting GILD data\n",
      "Getting GL data\n",
      "Getting GPN data\n",
      "Getting GS data\n",
      "Getting HAL data\n",
      "Getting HIG data\n",
      "Getting HAS data\n",
      "Getting HCA data\n",
      "Getting PEAK data\n",
      "Getting HSIC data\n",
      "Getting HSY data\n",
      "Getting HES data\n",
      "Getting HPE data\n",
      "Getting HLT data\n",
      "Getting HOLX data\n",
      "Getting HD data\n",
      "Getting HON data\n",
      "Getting HRL data\n",
      "Getting HST data\n",
      "Getting HWM data\n",
      "Getting HPQ data\n",
      "Getting HUM data\n",
      "Getting HII data\n",
      "Getting HBAN data\n",
      "Getting IBM data\n",
      "Getting IEX data\n",
      "Getting IDXX data\n",
      "Getting ITW data\n",
      "Getting ILMN data\n",
      "Getting INCY data\n",
      "Getting IR data\n",
      "Getting INTC data\n",
      "Getting ICE data\n",
      "Getting IP data\n",
      "Getting IPG data\n",
      "Getting IFF data\n",
      "Getting INTU data\n",
      "Getting ISRG data\n",
      "Getting IVZ data\n",
      "Getting IPGP data\n",
      "Getting IQV data\n",
      "Getting IRM data\n",
      "Getting JBHT data\n",
      "Getting JKHY data\n",
      "Getting J data\n",
      "Getting JNJ data\n",
      "Getting JCI data\n",
      "Getting JPM data\n",
      "Getting JNPR data\n",
      "Getting K data\n",
      "Getting KEY data\n",
      "Getting KEYS data\n",
      "Getting KMB data\n",
      "Getting KIM data\n",
      "Getting KMI data\n",
      "Getting KLAC data\n",
      "Getting KHC data\n",
      "Getting KR data\n",
      "Getting LHX data\n",
      "Getting LH data\n",
      "Getting LRCX data\n",
      "Getting LW data\n",
      "Getting LVS data\n",
      "Getting LDOS data\n",
      "Getting LEN data\n",
      "Getting LNC data\n",
      "Getting LIN data\n",
      "Getting LYV data\n",
      "Getting LKQ data\n",
      "Getting LMT data\n",
      "Getting L data\n",
      "Getting LOW data\n",
      "Getting LUMN data\n",
      "Getting LYB data\n",
      "Getting MTB data\n",
      "Getting MRO data\n",
      "Getting MPC data\n",
      "Getting MKTX data\n",
      "Getting MAR data\n",
      "Getting MMC data\n",
      "Getting MLM data\n",
      "Getting MAS data\n",
      "Getting MA data\n",
      "Getting MTCH data\n",
      "Getting MKC data\n",
      "Getting MCD data\n",
      "Getting MCK data\n",
      "Getting MDT data\n",
      "Getting MRK data\n",
      "Getting MET data\n",
      "Getting MTD data\n",
      "Getting MGM data\n",
      "Getting MCHP data\n",
      "Getting MU data\n",
      "Getting MSFT data\n",
      "Getting MAA data\n",
      "Getting MRNA data\n",
      "Getting MHK data\n",
      "Getting MOH data\n",
      "Getting TAP data\n",
      "Getting MDLZ data\n",
      "Getting MPWR data\n",
      "Getting MNST data\n",
      "Getting MCO data\n",
      "Getting MS data\n",
      "Getting MOS data\n",
      "Getting MSI data\n",
      "Getting MSCI data\n",
      "Getting NDAQ data\n",
      "Getting NTAP data\n",
      "Getting NFLX data\n",
      "Getting NWL data\n",
      "Getting NEM data\n",
      "Getting NWSA data\n",
      "Getting NWS data\n",
      "Getting NEE data\n",
      "Getting NLSN data\n",
      "Getting NKE data\n",
      "Getting NI data\n",
      "Getting NDSN data\n",
      "Getting NSC data\n",
      "Getting NTRS data\n",
      "Getting NOC data\n",
      "Getting NLOK data\n",
      "Getting NCLH data\n",
      "Getting NRG data\n",
      "Getting NUE data\n",
      "Getting NVDA data\n",
      "Getting NVR data\n",
      "Getting NXPI data\n",
      "Getting ORLY data\n",
      "Getting OXY data\n",
      "Getting ODFL data\n",
      "Getting OMC data\n",
      "Getting OKE data\n",
      "Getting ORCL data\n",
      "Getting OTIS data\n",
      "Getting PCAR data\n",
      "Getting PKG data\n",
      "Getting PH data\n",
      "Getting PAYX data\n",
      "Getting PAYC data\n",
      "Getting PYPL data\n",
      "Getting PENN data\n",
      "Getting PNR data\n",
      "Getting PEP data\n",
      "Getting PKI data\n",
      "Getting PFE data\n",
      "Getting PM data\n",
      "Getting PSX data\n",
      "Getting PNW data\n",
      "Getting PXD data\n",
      "Getting PNC data\n",
      "Getting POOL data\n",
      "Getting PPG data\n",
      "Getting PPL data\n",
      "Getting PFG data\n",
      "Getting PG data\n",
      "Getting PGR data\n",
      "Getting PLD data\n",
      "Getting PRU data\n",
      "Getting PEG data\n",
      "Getting PTC data\n",
      "Getting PSA data\n",
      "Getting PHM data\n",
      "Getting PVH data\n",
      "Getting QRVO data\n",
      "Getting PWR data\n",
      "Getting QCOM data\n",
      "Getting DGX data\n",
      "Getting RL data\n",
      "Getting RJF data\n",
      "Getting RTX data\n",
      "Getting O data\n",
      "Getting REG data\n",
      "Getting REGN data\n",
      "Getting RF data\n",
      "Getting RSG data\n",
      "Getting RMD data\n",
      "Getting RHI data\n",
      "Getting ROK data\n",
      "Getting ROL data\n",
      "Getting ROP data\n",
      "Getting ROST data\n",
      "Getting RCL data\n",
      "Getting SPGI data\n",
      "Getting CRM data\n",
      "Getting SBAC data\n",
      "Getting SLB data\n",
      "Getting STX data\n",
      "Getting SEE data\n",
      "Getting SRE data\n",
      "Getting NOW data\n",
      "Getting SHW data\n",
      "Getting SBNY data\n",
      "Getting SPG data\n",
      "Getting SWKS data\n",
      "Getting SJM data\n",
      "Getting SNA data\n",
      "Getting SEDG data\n",
      "Getting SO data\n",
      "Getting LUV data\n",
      "Getting SWK data\n",
      "Getting SBUX data\n",
      "Getting STT data\n",
      "Getting STE data\n",
      "Getting SYK data\n",
      "Getting SIVB data\n",
      "Getting SYF data\n",
      "Getting SNPS data\n",
      "Getting SYY data\n",
      "Getting TMUS data\n",
      "Getting TROW data\n",
      "Getting TTWO data\n",
      "Getting TPR data\n",
      "Getting TGT data\n",
      "Getting TEL data\n",
      "Getting TDY data\n",
      "Getting TFX data\n",
      "Getting TER data\n",
      "Getting TSLA data\n",
      "Getting TXN data\n",
      "Getting TXT data\n",
      "Getting TMO data\n",
      "Getting TJX data\n",
      "Getting TSCO data\n",
      "Getting TT data\n",
      "Getting TDG data\n",
      "Getting TRV data\n",
      "Getting TRMB data\n",
      "Getting TFC data\n",
      "Getting TWTR data\n",
      "Getting TYL data\n",
      "Getting TSN data\n",
      "Getting USB data\n",
      "Getting UDR data\n",
      "Getting ULTA data\n",
      "Getting UAA data\n",
      "Getting UA data\n",
      "Getting UNP data\n",
      "Getting UAL data\n",
      "Getting UPS data\n",
      "Getting URI data\n",
      "Getting UNH data\n",
      "Getting UHS data\n",
      "Getting VLO data\n",
      "Getting VTR data\n",
      "Getting VRSN data\n",
      "Getting VRSK data\n",
      "Getting VZ data\n",
      "Getting VRTX data\n",
      "Too may calls, delaying for thirteen seconds and retrying\n",
      "Getting VRTX data\n",
      "Getting VFC data\n",
      "Getting VTRS data\n",
      "Getting VICI data\n",
      "Getting V data\n",
      "Getting VNO data\n",
      "Getting VMC data\n",
      "Getting WAB data\n",
      "Getting WBA data\n",
      "Getting WMT data\n",
      "Getting WM data\n",
      "Getting WAT data\n",
      "Getting WEC data\n",
      "Getting WFC data\n",
      "Getting WELL data\n",
      "Getting WST data\n",
      "Getting WDC data\n",
      "Getting WRK data\n",
      "Getting WY data\n",
      "Getting WHR data\n",
      "Getting WMB data\n",
      "Getting GWW data\n",
      "Getting WYNN data\n",
      "Getting XEL data\n",
      "Getting XYL data\n",
      "Getting YUM data\n",
      "Getting ZBRA data\n",
      "Getting ZBH data\n",
      "Getting ZION data\n",
      "Getting ZTS data\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#Calls the 'get_ticker_data' function for every ticker in the SP500 (excluding some oddities)\n",
    "def acquire_all_data(start, end):\n",
    "    \n",
    "    '''for ticker in range(0, len(tickers)):\n",
    "        try:\n",
    "            mdf, ddf = get_ticker_data(tickers[ticker], start, end)\n",
    "            mdf.to_csv(f'MINUTE_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "            ddf.to_csv(f'DAILY_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "        except:\n",
    "            print(\"Too may calls, delaying for thirteen seconds and retrying\")\n",
    "            time.sleep(13)\n",
    "            try:\n",
    "                mdf, ddf = get_ticker_data(tickers[ticker], start, end)\n",
    "                mdf.to_csv(f'MINUTE_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "                ddf.to_csv(f'DAILY_STOCK_DATA//{tickers[ticker]}.csv')\n",
    "            except:\n",
    "                print(f\"Unable to acquire ticker {tickers[ticker]} daily/minute data\")\n",
    "            continue\n",
    "            '''\n",
    "    #Gets minute data including after hours\n",
    "    for ticker in range(191, len(tickers)):\n",
    "        try:\n",
    "            mdfaf= get_ticker_data(tickers[ticker], start, end, include_daily = False, ignore_after_hours = False)\n",
    "            mdfaf.to_csv(f'MINUTE_STOCK_DATA_AFTERHOURS//{tickers[ticker]}.csv')\n",
    "        except:\n",
    "            print(\"Too may calls, delaying for thirteen seconds and retrying\")\n",
    "            time.sleep(13)\n",
    "            try:\n",
    "                mdfaf = get_ticker_data(tickers[ticker], start, end, include_daily = False, ignore_after_hours = False)\n",
    "                mdfaf.to_csv(f'MINUTE_STOCK_DATA_AFTERHOURS//{tickers[ticker]}.csv')\n",
    "            except:\n",
    "                print(f\"Unable to acquire ticker {tickers[ticker]} after hours data\")\n",
    "            continue\n",
    "    print('Complete')\n",
    "\n",
    "acquire_all_data(start = '2017-07-08', end = '2022-07-24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724704a-6abe-49b2-8431-b700b59706bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
